# log dir 
log_dir: /home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/logs/vit

# model setting -> old
freeze: false
pretrained: #/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/pretrained/resnet_34-a63425a03e.pth
#/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/weights/ResNet34_Bcos_v2/resnet_34-a63425a03e.pth   # path to a pre-trained model, if using one
from_url: false
model_name: vit_bcos   # model name
backbone_name: vit_bcos  # backbone name

#backbone setting
backbone_config:
  num_classes: 2
  channels: 6
  #pretrained: false
  logit_bias: #0.0 #None
  logit_temperature: #10 #None
  b: 1.25
  max_out: 2

  # either set the model type
  model_type: simple_vit_ti_patch16_224

  # or the parameters directly, these values are only taken when model_type is not provided
  image_size: 14 #14 # Union[int, Tuple[int, int]],
  patch_size: 1 # Union[int, Tuple[int, int]],
  dim: 384 #192
  depth: 11 # 12-1
  heads: 6
  mlp_dim: 1536
  conv_stem: [48, 96, 192, 384] #[24, 48, 96, 192]

# dataset
all_dataset: [FaceForensics++, FF-F2F, FF-DF, FF-FS, FF-NT] #, FaceShifter, DeepFakeDetection, Celeb-DF-v1, Celeb-DF-v2, DFDCP, DFDC, DeeperForensics-1.0, UADFV]
train_dataset: [FaceForensics++]
val_dataset: [FaceForensics++] #, FF-F2F, FF-DF, FF-FS, FF-NT]
test_dataset: [FaceForensics++] #, FF-F2F, FF-DF, FF-FS, FF-NT]

compression: c40  # compression-level for videos
train_batchSize: 64   # training batch size
test_batchSize: 64   # test batch size
val_batchSize: 64
workers: 8   # number of data loading workers
frame_num: {'train': 32, 'test': 32, 'val': 32}   # number of frames to use per video in training and testing
resolution: 224   # resolution of output image to network
with_mask: false   # whether to include mask information in the input
with_landmark: false   # whether to include facial landmark information in the input
save_ckpt: true   # whether to save checkpoint
save_feat: true   # whether to save features
# add_inverse_channels: true


# data augmentation
dataset_type: 'bcos'
use_data_augmentation: true  # Add this flag to enable/disable data augmentation
data_aug:
  flip_prob: 0.5
  rotate_prob: 0.5
  rotate_limit: [-10, 10]
  blur_prob: 0.5
  blur_limit: [3, 7]
  brightness_prob: 0.5
  brightness_limit: [-0.1, 0.1]
  contrast_limit: [-0.1, 0.1]
  quality_lower: 40
  quality_upper: 100

# mean and std for normalization
mean: [0.5, 0.5, 0.5]
std: [0.5, 0.5, 0.5]

# optimizer config
optimizer:
  # choose between 'adam' and 'sgd'
  type: adam
  adam:
    lr: 0.000025  # learning rate -> bcos paper 2.5e-4
    beta1: 0.8  # beta1 for Adam optimizer
    beta2: 0.999 # beta2 for Adam optimizer
    eps: 0.000001  # epsilon for Adam optimizer (KAI: added in denominator to resolve division by zero if too low may cause NaNs!!)
    weight_decay: 0  # weight decay for regularization (KAI -> according to bcos paper, not necessary since weights normalized to unit norm) 
    amsgrad: true
  sgd:
    lr: 0.0002  # learning rate, KAI: Lower LR may stabilize the results
    momentum: 0.9  # momentum for SGD optimizer
    weight_decay: 0.0005  # weight decay for regularization
  gradient_clipping: false

# training config
lr_scheduler: warmup_cosine # step, cosine, warmup_cosine, linear
lr_eta_min: 0
lr_T_max: 200
# lr_step: 4
# lr_gamma: 0.2
nEpochs: 100   # number of epochs to train for
start_epoch: 0   # manual epoch number (useful for restarts)
save_epoch: 1   # interval epochs for saving models
rec_iter: 500   # interval iterations for recording
logdir: ./logs   # folder to output images and logs
manualSeed: 1024   # manual seed for random number generation
save_ckpt: false   # whether to save checkpoint

# loss function
loss_func: cross_entropy   # loss function to use
losstype: null

# metric
metric_scoring: auc   # metric for evaluation (auc, acc, eer, ap)

# cuda
ddp: false   # Kai: unnecessary, since it is overwritten anyway in train.py-> main() -> config['ddp']= args.ddp
cuda: true   # whether to use CUDA acceleration
cudnn: true   # whether to use CuDNN for convolution operations
