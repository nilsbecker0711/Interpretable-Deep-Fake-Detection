{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb3d9f0-422d-44d5-aac7-6d0de658127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x14d10c151ee0>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8042a3-6153-4db9-883e-6d32f2c43c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 09:37:44.710093: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 09:37:45.569654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-31 09:37:45.569694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-31 09:37:45.569727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 09:37:45.917388: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-31 09:38:13.959605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\"train.py\"]\n",
    "import logging\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "from B_COS_eval import BCOSEvaluator\n",
    "from LIME_eval import LIMEEvaluator  \n",
    "# from GRADCAM_eval import GradCamEvaluator  # Uncomment if implemented.\n",
    "from training.detectors.xception_detector import XceptionDetector\n",
    "from training.detectors import DETECTOR\n",
    "from dataset.abstract_dataset import DeepfakeAbstractBaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28902e55-d008-439a-b341-48edef3e9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(34)\n",
    "random.seed(34)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Disable for strict reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d79aaf6-10e8-47e0-b3da-370eb3ec066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default model configuration.\n",
    "MODEL_PATH = os.path.join(\"/pfs/data5/home/ma/ma_ma/ma_lineuman/Interpretable-Deep-Fake-Detection\", \"training/config/detector/resnet34_bcos_v2_minimal.yaml\")\n",
    "ADDITIONAL_ARGS = {\n",
    "    \"test_batchSize\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfa2ae6-1758-42cd-b4f4-93dbc5a37a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging.\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd6f73d-3485-454e-9ced-532969552c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    \"\"\"Load and return the model using the DETECTOR registry.\"\"\"\n",
    "    logger.info(\"Registered models: %s\", list(DETECTOR.data.keys()))\n",
    "    model_class = DETECTOR[config['model_name']]\n",
    "    model = model_class(config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7144c6d1-8d5b-4cff-b746-ce8cd5b44354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path, additional_args={}):\n",
    "    \"\"\"Load and merge configuration files with any additional overrides.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    try:\n",
    "        with open('./training/config/test_config.yaml', 'r') as f:\n",
    "            config2 = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        with open(os.path.expanduser('~/Interpretable-Deep-Fake-Detection/training/config/test_config.yaml'), 'r') as f:\n",
    "            config2 = yaml.safe_load(f)\n",
    "    if 'label_dict' in config:\n",
    "        config2['label_dict'] = config['label_dict']\n",
    "    config.update(config2)\n",
    "    if config.get('dry_run', False):\n",
    "        config['nEpochs'] = 0\n",
    "        config['save_feat'] = False\n",
    "    for key, value in additional_args.items():\n",
    "        config[key] = value\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d2ceb1-4b9c-420f-bef3-6d2a6f164b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_dataloader(data_loader):\n",
    "    \"\"\"Extract and return all images from a DataLoader.\"\"\"\n",
    "    all_images = []\n",
    "    for batch in data_loader:\n",
    "        images = batch['image']  # shape: [batch_size, C, H, W]\n",
    "        for img in images:\n",
    "            all_images.append(img)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bef7355-0c9a-41b7-823f-4784b7a3a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    # img is expected to be a tensor of shape [B, C, H, W] in a batch.\n",
    "    if img.shape[1] == 3:\n",
    "        img = torch.cat([img, 1.0 - img], dim=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c660b2dd-2f97-4a6b-82db-cc898303db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyser:\n",
    "    def analysis(self):\n",
    "        raise NotImplementedError(\"Need to implement analysis function.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Run analysis and save both raw and overall results.\n",
    "        overall, raw = self.analysis()\n",
    "        self.save_results(raw, overall)\n",
    "\n",
    "    def save_results(self, raw, overall):\n",
    "        \"\"\"Save results to a designated folder.\"\"\"\n",
    "        save_folder = os.path.join(\"results\", str(self.grid_split), f\"{self.model_name}_{self.weights_name}\")\n",
    "        os.makedirs(save_folder, exist_ok=True)  # Create directory if needed.\n",
    "        with open(os.path.join(save_folder, \"results.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(raw, f)  # Save raw results.\n",
    "        with open(os.path.join(save_folder, \"overall.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(overall, f)  # Save overall metrics.\n",
    "        logger.info(\"Results saved to folder: %s\", save_folder)\n",
    "        \n",
    "    def load_results(self, load_overall=True):\n",
    "        \"\"\"Load results from disk and print summary info.\"\"\"\n",
    "        load_folder = os.path.join(\"results\", str(self.grid_split), f\"{self.model_name}_{self.weights_name}\")\n",
    "        file_path = os.path.join(load_folder, \"overall.pkl\" if load_overall else \"results.pkl\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            loaded = pickle.load(f)\n",
    "        logger.info(\"Results loaded from %s\", file_path)\n",
    "        if load_overall:\n",
    "            localisation_metric = loaded.get(\"localisation_metric\", None)\n",
    "            percentiles = loaded.get(\"percentiles\", None)\n",
    "            if localisation_metric is not None and percentiles is not None:\n",
    "                logger.info(\"Overall results: %d evaluations, percentiles: %s\", len(localisation_metric), percentiles)\n",
    "            else:\n",
    "                logger.warning(\"Overall results missing expected keys.\")\n",
    "        else:\n",
    "            sorted_results = sorted(loaded, key=lambda res: res.get(\"accuracy\", 0), reverse=True)\n",
    "            logger.info(\"Top raw results:\")\n",
    "            for idx, res in enumerate(sorted_results[:10]):\n",
    "                logger.info(\"[%d] %s - Accuracy: %s\", idx + 1, res.get(\"path\", \"N/A\"), res.get(\"accuracy\", \"N/A\"))\n",
    "        return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe7ff8b-08ad-4cc3-aad3-89fa333b1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedGPGCreator(Analyser):\n",
    "    def __init__(self, base_output_dir, grid_size=(3, 3), xai_method=None, max_grids=3, plotting_only=False,\n",
    "                 model=None, model_name=\"default\", weights_name=\"default\",\n",
    "                 test_data_loaders=None, dataset=None, device=None, config=None, grid_split=3):\n",
    "        \"\"\"\n",
    "        Initialize grid creator with specified parameters.\n",
    "        base_output_dir: Base directory for grids.\n",
    "        grid_size: Dimensions of the grid (e.g., (3,3)).\n",
    "        xai_method: \"bcos\", \"lime\", or \"gradcam\".\n",
    "        max_grids: Maximum number of grids to create.\n",
    "        plotting_only: If True, load existing results.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.xai_method = xai_method\n",
    "        self.max_grids = max_grids\n",
    "        self.model = model\n",
    "        self.test_data_loaders = test_data_loaders\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.weights_name = weights_name\n",
    "        self.output_folder = os.path.join(base_output_dir, f\"{model_name}_{weights_name}\")\n",
    "        self.device = device\n",
    "        self.grid_split = grid_split\n",
    "\n",
    "        if plotting_only:\n",
    "            self.load_results()\n",
    "            return\n",
    "        \n",
    "        # Create output directory for grids.\n",
    "        self.output_dir = os.path.join(self.output_folder, str(self.grid_split))\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Load or compute sorted image rankings.\n",
    "        self.ranking_file = os.path.join(self.output_folder, \"sorted_confs.pkl\")\n",
    "        if os.path.exists(self.ranking_file):\n",
    "            self.sorted_confs = self.load_ranking(self.ranking_file)\n",
    "            logger.info(\"Loaded sorted confidences from %s\", self.ranking_file)\n",
    "        else:\n",
    "            self.sorted_confs = self.compute_sorted_confs()\n",
    "            self.save_ranking(self.sorted_confs, self.ranking_file)\n",
    "            logger.info(\"Saved sorted confidences to %s\", self.ranking_file)\n",
    "\n",
    "    def compute_sorted_confs(self):\n",
    "        \"\"\"Compute ranking by storing (image_path, confidence, label) for each correctly classified image.\"\"\"\n",
    "        ranking = {0: [], 1: []}\n",
    "        key = list(self.test_data_loaders.keys())[0]\n",
    "        for data_dict in self.test_data_loaders[key]:\n",
    "            # Move all tensor values in data_dict to the device first.\n",
    "            for k, value in data_dict.items():\n",
    "                if value is not None and hasattr(value, 'to'):\n",
    "                    data_dict[k] = value.to(self.device)\n",
    "            \n",
    "            # Now unpack after moving to device.\n",
    "            img_batch, label_batch, mask, landmark, path_of_image = (\n",
    "                data_dict[k] for k in ['image', 'label', 'mask', 'landmark', 'image_path']\n",
    "            )\n",
    "            \n",
    "            # Remove extra key if present.\n",
    "            data_dict.pop('label_spe', None)\n",
    "            # Convert labels to binary.\n",
    "            data_dict['label'] = torch.where(data_dict['label'] != 0, 1, 0)\n",
    "\n",
    "            num_samples = img_batch.shape[0]\n",
    "            # Process each image in the batch.\n",
    "            for j in range(num_samples):\n",
    "                image = img_batch[j].unsqueeze(0)  # shape: [1, C, H, W]\n",
    "                label = label_batch[j]\n",
    "                true_label = int(label.item())  # Convert label tensor to int.\n",
    "                image_path = path_of_image[j]\n",
    "                \n",
    "                if self.xai_method == \"bcos\":\n",
    "                    image = preprocess_image(image)\n",
    "                output = self.model({'image': image, 'label': label})\n",
    "                logit = output['cls']  # Expected shape: [1, num_classes]\n",
    "                # Get predicted label from the first (and only) sample.\n",
    "                predicted_label = logit[0].argmax().item()\n",
    "                # Compute confidence from the corresponding logit.\n",
    "                confidence = logit[0, predicted_label].item()\n",
    "                \n",
    "                # Check for class 1.\n",
    "                if true_label == 1:\n",
    "                    if predicted_label == 1:\n",
    "                        ranking[1].append((image_path, confidence, true_label))\n",
    "                # Check for class 0.\n",
    "                if true_label == 0:\n",
    "                    if predicted_label == 0:\n",
    "                        ranking[0].append((image_path, confidence, true_label))\n",
    "        \n",
    "        # Sort each class's ranking by descending confidence.\n",
    "        for cls in ranking:\n",
    "            ranking[cls] = sorted(ranking[cls], key=lambda x: x[1], reverse=True)\n",
    "            logger.debug(\"Class %d: %d images after sorting.\", cls, len(ranking[cls]))\n",
    "        return ranking\n",
    "    \n",
    "    def get_sorted_image_paths(self):\n",
    "        \"\"\"Select top image indices based on rankings for grid creation,\n",
    "        filtering each tuple by a confidence threshold (sigmoid(confidence) > 0.5).\n",
    "        For class 0 (real), selects k * (grid_size[0] * grid_size[1] - 1) images,\n",
    "        and for class 1 (fake), selects k images.\n",
    "        \"\"\"\n",
    "        # Helper function: returns True if sigmoid(confidence) > 0.5.\n",
    "        def get_conf_mask_v(tup):\n",
    "            # tup is (img_idx, confidence)\n",
    "            return torch.tensor(tup[1]).sigmoid().item() > 0.5\n",
    "    \n",
    "        k = self.max_grids\n",
    "        sorted_image_paths = {}\n",
    "        for cls in [0, 1]:\n",
    "            # Get the sorted list for this class (list of tuples: (img_idx, confidence))\n",
    "            cls_list = self.sorted_confs.get(cls, [])\n",
    "            # Filter the list by confidence threshold.\n",
    "            filtered = [tup for tup in cls_list if get_conf_mask_v(tup)]\n",
    "            # Determine the number of required images:\n",
    "            required = k * (self.grid_size[0] * self.grid_size[1] - 1) if cls == 0 else k\n",
    "            # Select only the image indices from the filtered list (up to the required number).\n",
    "            sorted_image_paths[cls] = filtered[:required]\n",
    "        return sorted_image_paths\n",
    "\n",
    "    def load_sample_by_path(self, image_path, expected_label):\n",
    "        try:\n",
    "            idx = self.image_list.index(image_path)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Image path {image_path} not found in dataset.\")\n",
    "        \n",
    "        sample = self.__getitem__(idx)  # (image, label, landmark, mask, stored_index)\n",
    "        sample_label = int(sample[1])  # assuming the second element is the label\n",
    "        if sample_label != expected_label:\n",
    "            raise ValueError(f\"Label mismatch at {image_path}: expected {expected_label} but got {sample_label}\")\n",
    "        return sample[0]  # or return the full sample as needed\n",
    "    \n",
    "    '''\n",
    "    def get_image_by_index(self, idx, expected_label=None):\n",
    "        \"\"\"Return image tensor from the dataset by direct indexing.\n",
    "        \n",
    "        Assumes __getitem__ returns (image, label, landmark, mask, stored_index).\n",
    "        Converts the dataset label to binary (0 for real, non-zero -> 1) before checking.\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "        stored_index = sample[-1]\n",
    "        if stored_index != idx:\n",
    "            raise ValueError(f\"Dataset index mismatch: expected {idx} but got {stored_index}\")\n",
    "        \n",
    "        sample_label = sample[1]\n",
    "        sample_label = int(sample_label)\n",
    "        sample_label = 0 if sample_label == 0 else 1\n",
    "        \n",
    "        if expected_label is not None and sample_label != expected_label:\n",
    "            raise ValueError(f\"Label mismatch at index {idx}: expected {expected_label} but got {sample_label}\")\n",
    "        \n",
    "        return sample[0]\n",
    "    '''\n",
    "    \n",
    "    def save_ranking(self, ranking, file_path):\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(ranking, f)\n",
    "\n",
    "    def load_ranking(self, file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            ranking = pickle.load(f)\n",
    "        return ranking\n",
    "    \n",
    "    def analysis(self):\n",
    "        \"\"\"Evaluate grid tensors and compute overall metrics.\"\"\"\n",
    "        results_folder = os.path.join(\"results\", str(self.grid_split), f\"{self.model_name}_{self.weights_name}\")\n",
    "        raw_results_file = os.path.join(results_folder, \"results.pkl\")\n",
    "        if os.path.exists(raw_results_file):\n",
    "            raise RuntimeError(f\"Results already exist at {raw_results_file}. Use load_results() instead.\")\n",
    "\n",
    "        # List grid tensor files.\n",
    "        grid_dir = os.path.join(self.output_folder, str(self.grid_split))\n",
    "        grid_paths = [os.path.join(grid_dir, f) for f in os.listdir(grid_dir) if f.endswith('.pt')]\n",
    "        logger.info(\"Found %d grid tensors in %s.\", len(grid_paths), grid_dir)\n",
    "\n",
    "        # Load each grid tensor.\n",
    "        preprocessed_tensors = [torch.load(path, map_location=self.device) for path in grid_paths]\n",
    "        logger.info(\"Loaded all grid tensors.\")\n",
    "\n",
    "        # Choose evaluator based on xai_method.\n",
    "        if self.xai_method == \"bcos\":\n",
    "            evaluator = BCOSEvaluator(self.model, self.device)\n",
    "        elif self.xai_method == \"lime\":\n",
    "            evaluator = LIMEEvaluator(self.model, self.device)\n",
    "        elif self.xai_method == \"gradcam\":\n",
    "            raise NotImplementedError(\"GradCAM evaluator not implemented.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown xai_method: {self.xai_method}\")\n",
    "\n",
    "        raw_results = evaluator.evaluate(preprocessed_tensors, grid_paths, self.grid_split)\n",
    "        grid_accuracies = [res[\"accuracy\"] for res in raw_results]\n",
    "        percentiles = np.percentile(np.array(grid_accuracies), [25, 50, 75, 100])\n",
    "        logger.info(\"Localisation accuracy percentiles: %s\", percentiles)\n",
    "        overall = {\"localisation_metric\": grid_accuracies, \"percentiles\": percentiles}\n",
    "        return overall, raw_results\n",
    "\n",
    "    def create_GPG_grids(self):\n",
    "        \"\"\"Create grids by combining ranked real and fake images.\"\"\"\n",
    "        logger.info(\"=== Starting GPG grid creation in %s ===\", self.output_folder)\n",
    "        \n",
    "        # Check if grids already exist.\n",
    "        existing_files = [f for f in os.listdir(self.output_dir) if f.endswith('.pt')]\n",
    "        logger.debug(\"Found %d existing .pt files in %s.\", len(existing_files), self.output_dir)\n",
    "        if len(existing_files) >= self.max_grids:\n",
    "            logger.info(\"Existing grid files found. Skipping grid creation.\")\n",
    "            return\n",
    "        \n",
    "        # Copy the ranked real/fake lists\n",
    "        #ranked_real = self.sorted_confs.get(0, []).copy()\n",
    "        #ranked_fake = self.sorted_confs.get(1, []).copy()\n",
    "\n",
    "        # Get sorted image paths (tuples of (image_path, confidence, label))\n",
    "        sorted_image_paths = self.get_sorted_image_paths()\n",
    "        # Expect one fake (class 1) and remaining real (class 0) images.\n",
    "        ranked_real = sorted_image_paths.get(0, []).copy()\n",
    "        ranked_fake = sorted_image_paths.get(1, []).copy()\n",
    "        logger.debug(\"Ranked real: %d, Ranked fake: %d\", len(ranked_real), len(ranked_fake))\n",
    "        \n",
    "        n_imgs = self.grid_size[0] * self.grid_size[1]\n",
    "        logger.debug(\"Total images per grid: %d\", n_imgs)\n",
    "        side = int(np.sqrt(n_imgs))\n",
    "        logger.debug(\"Calculated grid side length: %d\", side)\n",
    "        \n",
    "        grid_count = 0\n",
    "        while grid_count < self.max_grids:\n",
    "            logger.info(\"--- Creating grid %d of %d ---\", grid_count + 1, self.max_grids)\n",
    "            required_real = n_imgs - 1  # Reserve 1 slot for fake image.\n",
    "            fake_count = len(ranked_fake)\n",
    "            real_count = len(ranked_real)\n",
    "            logger.debug(\"Need %d real, have %d; need 1 fake, have %d.\", required_real, real_count, fake_count)\n",
    "            \n",
    "            if fake_count < 1 or real_count < required_real:\n",
    "                logger.warning(\"Not enough images: fake %d, real %d (required %d)\", fake_count, real_count, required_real)\n",
    "                break\n",
    "            \n",
    "            # Get first fake tuple and remove it.\n",
    "            fake_tuple = ranked_fake.pop(0)\n",
    "            logger.info(\"Selected fake image: %s with confidence %.4f\", fake_tuple[0], fake_tuple[1])\n",
    "            expected_label = 1\n",
    "            fake_img = self.load_sample_by_path(fake_tuple[0], expected_label)\n",
    "            logger.debug(\"Fake image shape: %s\", fake_img.shape if hasattr(fake_img, 'shape') else \"N/A\")\n",
    "            \n",
    "            # Select first required_real real image tuples.\n",
    "            selected_real_tuples = ranked_real[:required_real]\n",
    "            logger.info(\"Selected real image paths: %s\", selected_real_tuples)\n",
    "            ranked_real = ranked_real[required_real:]  # Remove used entries.\n",
    "            \n",
    "            # Retrieve real images using load_sample_by_path for consistency.\n",
    "            expected_label = 0\n",
    "            selected_real = [self.load_sample_by_path(img_path, expected_label) for img_path, _, _ in selected_real_tuples]\n",
    "            logger.debug(\"Retrieved %d real images.\", len(selected_real))\n",
    "            \n",
    "            # Combine real and fake images.\n",
    "            images = selected_real + [fake_img]\n",
    "            logger.debug(\"Combined image count: %d\", len(images))\n",
    "            random.shuffle(images)  # Shuffle grid placement.\n",
    "            logger.debug(\"Images shuffled.\")\n",
    "            \n",
    "            # Find fake image index in shuffled list.\n",
    "            fake_index = next(i for i, img in enumerate(images) if torch.equal(img, fake_img))\n",
    "            final_fake_index = (fake_index % side) * side + (fake_index // side)\n",
    "            logger.debug(\"Fake image: shuffled index %d, final index %d\", fake_index, final_fake_index)\n",
    "            \n",
    "            # Stack images and reshape into grid tensor.\n",
    "            stacked = torch.stack(images, dim=0)\n",
    "            logger.debug(\"Stacked images shape: %s\", stacked.shape)\n",
    "            grid_tensor = (\n",
    "                stacked.view(-1, side, side, *stacked.shape[-3:])\n",
    "                       .permute(0, 3, 2, 4, 1, 5)\n",
    "                       .reshape(-1, stacked.shape[1], stacked.shape[2] * side, stacked.shape[3] * side)\n",
    "            )\n",
    "            logger.debug(\"Grid tensor shape: %s\", grid_tensor.shape)\n",
    "            \n",
    "            # Save grid tensor with fake position encoded in filename.\n",
    "            base_name = f\"grid_{grid_count}_fake_{final_fake_index}.pt\"\n",
    "            path_to_grid = os.path.join(self.output_dir, base_name)\n",
    "            torch.save(grid_tensor, path_to_grid)\n",
    "            logger.info(\"Saved grid tensor: %s\", path_to_grid)\n",
    "            \n",
    "            grid_count += 1\n",
    "        \n",
    "        logger.info(\"=== Finished grid creation. Created %d grids. ===\", grid_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc58fa4-d941-4248-893a-9377feefe577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_output_dir='datasets/GPG_grids', max_grids=3, xai_method='bcos', model_path='/pfs/data5/home/ma/ma_ma/ma_lineuman/Interpretable-Deep-Fake-Detection/training/config/detector/resnet34_bcos_v2_minimal.yaml', grid_split=2)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Evaluate grids using a model and XAI method.\")\n",
    "parser.add_argument(\"--base_output_dir\", type=str, default=\"datasets/GPG_grids\",\n",
    "                    help=\"Base output directory for grids.\")\n",
    "parser.add_argument(\"--max_grids\", type=int, default=3, help=\"Max number of grids to create.\")\n",
    "parser.add_argument(\"--xai_method\", type=str, default=\"bcos\",\n",
    "                    choices=[\"bcos\", \"lime\", \"gradcam\"], help=\"XAI method to use.\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=MODEL_PATH,\n",
    "                    help=\"Path to model configuration file.\")\n",
    "parser.add_argument(\"--grid_split\", type=int, default=2,\n",
    "                    help=\"Grid split (e.g., 3 for a 3x3 grid).\")\n",
    "\n",
    "# Use parse_known_args to ignore the extra kernel arguments that Jupyter passes\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba4a727-5935-489b-b25b-1befd00d726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Parameters: XAI=bcos, Base=datasets/GPG_grids, Model=/pfs/data5/home/ma/ma_ma/ma_lineuman/Interpretable-Deep-Fake-Detection/training/config/detector/resnet34_bcos_v2_minimal.yaml, Grid=2x2\n"
     ]
    }
   ],
   "source": [
    "grid_size = (args.grid_split, args.grid_split)\n",
    "logger.info(\"Parameters: XAI=%s, Base=%s, Model=%s, Grid=%dx%d\",\n",
    "            args.xai_method, args.base_output_dir, args.model_path, args.grid_split, args.grid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87c2341-81b9-4297-be87-3e2f18401db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Registered models: ['resnet34', 'resnet34_bcos', 'resnet34_bcos_v2', 'resnet34_bcos_v2_minimal', 'inception_bcos_detector', 'xception', 'vit_bcos', 'convnext_bcos', 'vgg11_v2_bcos']\n",
      "INFO:training.detectors.resnet34_bcos_v2_minimal_detector:Load pretrained model successfully!\n",
      "INFO:__main__:Loaded model ResnetBcosDetector_v2_minimal on device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "config = load_config(args.model_path, additional_args=ADDITIONAL_ARGS)\n",
    "model = load_model(config)\n",
    "\n",
    "pretrained_path = config['pretrained']\n",
    "state_dict = torch.load(pretrained_path)\n",
    "# Remove \"module.\" prefix from state_dict keys if necessary.\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k.replace(\"module.\", \"\")] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Set device and move model.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(next(model.parameters()).device)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode.\n",
    "logger.info(\"Loaded model %s on device %s\", model.__class__.__name__, device)\n",
    "\n",
    "model_name = config.get(\"model_name\", \"defaultModel\")\n",
    "if not os.path.exists(pretrained_path):\n",
    "    raise FileNotFoundError(f\"Weight file not found: {pretrained_path}\")\n",
    "weights_name = os.path.basename(pretrained_path).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d72f4f-d504-413f-8677-c7fdb3a8c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method explain in module bcos.common:\n",
      "\n",
      "explain(in_tensor, idx=None, **grad2img_kwargs) -> 'Dict[str, Any]' method of training.networks.base.resnet34_bcos_v2_minimal.ResNet34_bcos_v2_minimal instance\n",
      "    Generates an explanation for the given input tensor.\n",
      "    This is not a generic explanation method, but rather a helper for simply getting explanations.\n",
      "    It is intended for simple use cases (simple exploration, debugging, etc.).\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    in_tensor : Tensor\n",
      "        The input tensor to explain. Must be 4-dimensional and have batch size of 1.\n",
      "    idx : int, optional\n",
      "        The index of the output to explain. If None, the prediction is explained.\n",
      "    grad2img_kwargs : Any\n",
      "        Additional keyword arguments passed to `gradient_to_image` method\n",
      "        for generating the explanation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Here is an example of how to use this method to generate and visualize an explanation and a contribution map:\n",
      "    \n",
      "    >>> model = ...  # instantiate some B-cos model\n",
      "    >>> img = ...  # instantiate some input image tensor\n",
      "    >>> expl_out = model.explain(img)\n",
      "    >>> expl_out[\"prediction\"]\n",
      "    932\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.imshow(expl_out[\"explanation\"])\n",
      "    >>> plt.show()  # show the explanation\n",
      "    \n",
      "    >>> model.plot_contribution_map(expl_out[\"contribution_map\"])\n",
      "    >>> plt.show()  # show the contribution map\n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    This method is NOT optimized for speed.\n",
      "    Also, on a more general note: Care should be taken when generating explanations during training,\n",
      "    as the gradients might be different from during inference.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Dict[str, Any]\n",
      "        A dictionary containing the explanation and additional information.\n",
      "        Namely, the following keys are present:\n",
      "        - \"prediction\": The prediction of the model.\n",
      "        - \"explained_class_idx\": The class (index) of the explained output.\n",
      "        - \"dynamic_linear_weights\": The dynamic linear weights of the model (`in_tensor.grad`).\n",
      "        - \"contribution_map\": The contribution map of the model prediction.\n",
      "        - \"explanation\": The explanation of the model prediction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.backbone.explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cd5f42-e000-4f91-b3da-6e6d1480c878",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_testing_data\n\u001b[0;32m----> 6\u001b[0m test_data_loaders \u001b[38;5;241m=\u001b[39m prepare_testing_data(\u001b[43mconfig\u001b[49m)\n\u001b[1;32m      7\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_data_loaders\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Reset sys.argv so that only the script name is present.\n",
    "sys.argv = [sys.argv[0]]\n",
    "\n",
    "from train import prepare_testing_data\n",
    "test_data_loaders = prepare_testing_data(config)\n",
    "test_loader = list(test_data_loaders.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a5697-b04b-4fe9-baa4-b9ed148ba9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=test_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ad550-dd14-46e3-9e1c-151d732e37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    sample = dataset[i]\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d38c3-2339-4110-8283-eac207571554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize grid creator with all required objects.\n",
    "grid_creator = RankedGPGCreator(\n",
    "    base_output_dir=args.base_output_dir,\n",
    "    grid_size=grid_size,\n",
    "    xai_method=args.xai_method,\n",
    "    max_grids=args.max_grids,\n",
    "    model=model,\n",
    "    model_name=model_name,\n",
    "    weights_name=weights_name,\n",
    "    test_data_loaders= test_data_loaders,\n",
    "    #loader=test_loader,\n",
    "    dataset=test_loader.dataset,\n",
    "    device=device,\n",
    "    grid_split=args.grid_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da452c62-923a-42e3-b577-94cb84bb8d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceForensics++\n",
      "dict_keys(['image', 'label', 'landmark', 'mask', 'index'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      5\u001b[0m data, label, mask, landmark, indices \u001b[38;5;241m=\u001b[39m (data_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandmark\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch of images shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_batch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_batch' is not defined"
     ]
    }
   ],
   "source": [
    "key = list(test_data_loaders.keys())[0]\n",
    "for i, data_dict in enumerate(test_data_loaders[key]):\n",
    "    print(key)\n",
    "    print(data_dict.keys())\n",
    "    data, label, mask, landmark, indices = (data_dict[k] for k in ['image', 'label', 'mask', 'landmark', 'index'])\n",
    "    print(f\"Batch of images shape: {img_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e88cbf-15a7-490e-8c29-f36eaf32d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== Starting GPG grid creation in datasets/GPG_grids/resnet34_bcos_v2_minimal_bcos_resnet_minimal_b2_ckpt_best ===\n",
      "INFO:__main__:--- Creating grid 1 of 2 ---\n",
      "WARNING:__main__:Not enough images to create more grids. Fake count: 17864, Real count: 0 (required 8 real images)\n",
      "INFO:__main__:=== Finished GPG grid creation. Created 0 grids. ===\n"
     ]
    }
   ],
   "source": [
    "grid_creator.create_GPG_grids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac101055-8735-44e6-bc2a-a747ea9653c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 2 grid tensors in datasets/GPG_grids/resnet34_bcos_v2_minimal_bcos_resnet_minimal_b2_ckpt_best/3ch.\n",
      "INFO:__main__:Loaded all grid tensors.\n",
      "INFO:root:Loaded model ResnetBcosDetector_v2_minimal onto cuda\n",
      "INFO:root:Processing 2 grids with grid_split=3.\n",
      "INFO:root:Evaluating grid 0 from file: datasets/GPG_grids/resnet34_bcos_v2_minimal_bcos_resnet_minimal_b2_ckpt_best/3ch/grid_1_fake_0.pt\n",
      "INFO:root:For grid grid_1_fake_0.pt: true position 0, predicted 4, grid accuracy: 0.109\n",
      "INFO:root:Evaluating grid 1 from file: datasets/GPG_grids/resnet34_bcos_v2_minimal_bcos_resnet_minimal_b2_ckpt_best/3ch/grid_0_fake_7.pt\n",
      "INFO:root:For grid grid_0_fake_7.pt: true position 7, predicted 1, grid accuracy: 0.108\n",
      "INFO:__main__:Localisation accuracy percentiles: [0.10847886 0.10866199 0.10884511 0.10902824]\n",
      "INFO:__main__:Results saved to folder: results/GPG/resnet34_bcos_v2_minimal_bcos_resnet_minimal_b2_ckpt_best\n"
     ]
    }
   ],
   "source": [
    "grid_creator.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc47090-83d1-44d7-80ca-23e04427dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_creator.load_results(load_overall=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
