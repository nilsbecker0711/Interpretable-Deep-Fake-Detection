{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection\")\n",
    "sys.path.append(\"/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2\")\n",
    "#sys.path.append(\"/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/training\")\n",
    "sys.argv = [\"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lime/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/lime/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/lime/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/lime/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/lime/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/lime/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/lime/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from training.detectors import DETECTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_grad_cam import XGradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Xception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered models: dict_keys(['vgg19_bcos', 'resnet34', 'resnet34_bcos', 'resnet34_bcos_v2', 'inception_bcos_detector', 'xception'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/training/detectors/xception_detector.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(config['pretrained'], map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XceptionDetector(\n",
       "  (backbone): Xception(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (block1): Block(\n",
       "      (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): SeparableConv2d(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): SeparableConv2d(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block3): Block(\n",
       "      (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block4): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block6): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block7): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block8): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block9): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block10): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block11): Block(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block12): Block(\n",
       "      (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (conv3): SeparableConv2d(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): SeparableConv2d(\n",
       "      (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (last_linear): Linear(in_features=2048, out_features=2, bias=True)\n",
       "    (adjust_channel): Sequential(\n",
       "      (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (loss_func): CrossEntropyLoss(\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from training.detectors import xception_detector\n",
    "#i needed to append my analysis path to system to access the bcos saved models\n",
    "\n",
    "def load_config(path, additional_args = {}):\n",
    "    # parse options and load config\n",
    "    with open(path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    try:# KAI: added this, to ensure it finds the config file\n",
    "        #with open('/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection/training/config/train_config.yaml', 'r') as f:\n",
    "        with open('/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/training/config/train_config.yaml', 'r') as f:\n",
    "            config2 = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        #with open(os.path.expanduser('/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection/training/config/train_config.yaml'), 'r') as f:\n",
    "        with open(os.path.expanduser('/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/training/config/train_config.yaml'), 'r') as f:\n",
    "            config2 = yaml.safe_load(f)\n",
    "    if 'label_dict' in config:\n",
    "        config2['label_dict']=config['label_dict']\n",
    "    config.update(config2)\n",
    "    # config['local_rank']=args.local_rank\n",
    "    if config['dry_run']:\n",
    "        config['nEpochs'] = 0\n",
    "        config['save_feat']=False\n",
    "    for key, value in additional_args.items():\n",
    "        config[key] = value\n",
    "    return config\n",
    "\n",
    "#path = \"/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection/training/config/detector/xception.yaml\"\n",
    "path = \"/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/training/config/detector/xception.yaml\"\n",
    "additional_args = {'test_batchSize': 12, \n",
    "                   'pretrained':\n",
    "                    #f'/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection/weights/resnet/ckpt_best.pth'\n",
    "                    f'/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/weights/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-07-10-17-17/test/avg/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-07-12-19-31/test/avg/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-07-17-23-41/test/avg/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-07-21-26-03/test/avg/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-07-21-43-33/test/avg/ckpt_best.pth'\n",
    "                    #f'{weights_base_path}xception_2025-02-08-09-13-00/test/avg/ckpt_best.pth'\n",
    "                  }\n",
    "config = load_config(path, additional_args=additional_args)\n",
    "print(\"Registered models:\", DETECTOR.data.keys())\n",
    "model_class = DETECTOR[config['model_name']]\n",
    "model = model_class(config)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([20, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "class MyToTensor(transforms.ToTensor):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Same as transforms.ToTensor, except that if input to __call__ is already tensor, the input is returned unchanged\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, input_img):\n",
    "        if not isinstance(input_img, torch.Tensor):\n",
    "            return super().__call__(input_img)\n",
    "        return input_img\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize if necessary\n",
    "    MyToTensor(),            # Converts image to tensor if not already          \n",
    "])\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_paths (dict): Dictionary where keys are folder paths, and values are the labels.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_files = []  # Store (image_path, label) tuples\n",
    "        for fp, label in folder_paths.items():\n",
    "            # List all subdirectories within the folder\n",
    "            subfolders = [os.path.join(fp, d) for d in os.listdir(fp) if os.path.isdir(os.path.join(fp, d))]\n",
    "        # Include images from the root directory as well as subfolders\n",
    "        self.image_files.extend(\n",
    "            [(os.path.join(fp, f), label) for f in os.listdir(fp) if f.endswith((\".png\", \".jpg\"))]\n",
    "        )\n",
    "\n",
    "        for folder_path in subfolders:\n",
    "            self.image_files.extend(\n",
    "                [(os.path.join(folder_path, f), label) for f in os.listdir(folder_path) if f.endswith((\".png\", \".jpg\"))]\n",
    "            )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_files[idx]  # Get image path and its label\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, img_path\n",
    "\n",
    "\n",
    "#file_path_deepfakebench = {\"/Users/Linus/Desktop/GIThubXAIFDEEPFAKE/Interpretable-Deep-Fake-Detection/datasets/GPG_grids/3ch\": 1}\n",
    "file_path_deepfakebench = {\"/Users/msrobin/GitHub Repositorys/Interpretable-Deep-Fake-Detection-2/datasets/2x2_images\": 1}\n",
    "\n",
    "dataset = CustomImageDataset(file_path_deepfakebench, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, label, img_path in dataloader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([20, 3, 224, 224])\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n",
      "predicted prob tensor([0.5043], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for img_batch, label_batch, path_batch in dataloader:\n",
    "    print(f\"Batch of images shape: {img_batch.shape}\")\n",
    "\n",
    "    # Iterate through the first 5 images in the batch\n",
    "    num_images = min(len(img_batch), 20)\n",
    "    for i in range(num_images):\n",
    "        img = img_batch[i].unsqueeze(0).to(device)  # Process a single image\n",
    "        label = label_batch[i]\n",
    "        img_path = path_batch[i]\n",
    "        img = img.requires_grad_(True)\n",
    "        \n",
    "        data_dict = {'image': img, 'label': label}\n",
    "        \n",
    "        model.zero_grad()\n",
    "        out = model(data_dict)\n",
    "        \n",
    "        prob = out['prob']\n",
    "        print('predicted prob', prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find last layer automatically \n",
    "def find_last_conv_layer(model):\n",
    "    \"\"\"Finds the last convolutional layer in the model.\"\"\"\n",
    "    for layer in reversed(list(model.modules())):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            return layer\n",
    "    return None  # Fallback in case no Conv2d is found\n",
    "\n",
    "\n",
    "TARGET_LAYER = find_last_conv_layer(model.backbone)\n",
    "\n",
    "# Check if a valid layer was found\n",
    "if TARGET_LAYER is None:\n",
    "    raise ValueError(\"No convolutional layer found in the model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([20, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 224, 224])\n",
      "[<pytorch_grad_cam.utils.model_targets.ClassifierOutputTarget object at 0x317cb4370>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(targets)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run Grad-CAM\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Extract first image's CAM\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# After Grad-CAM, squeeze only the batch dim\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(grayscale_cam)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lime/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:209\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lime/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:109\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_gradients:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([target(output) \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetach:\n\u001b[1;32m    111\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lime/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_gradients:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetach:\n\u001b[1;32m    111\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lime/lib/python3.9/site-packages/pytorch_grad_cam/utils/model_targets.py:11\u001b[0m, in \u001b[0;36mClassifierOutputTarget.__call__\u001b[0;34m(self, model_output)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_output):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "cam = XGradCAM(model=model, target_layers=[TARGET_LAYER])\n",
    "\n",
    "for img_batch, label_batch, path_batch in dataloader:\n",
    "    print(f\"Batch of images shape: {img_batch.shape}\")\n",
    "\n",
    "    # Iterate through the first 5 images in the batch\n",
    "    num_images = min(len(img_batch), 20)\n",
    "    for i in range(num_images):\n",
    "        img = img_batch[i].unsqueeze(0).to(device)  # Process a single image\n",
    "        label = label_batch[i]\n",
    "        img_path = path_batch[i]\n",
    "        print(img.shape)\n",
    "\n",
    "        img_np = np.transpose(img[0].cpu().numpy(), (1, 2, 0))  # Convert to NumPy\n",
    "        img = img.requires_grad_(True)\n",
    "\n",
    "        # Prepare input for XGrad-CAM\n",
    "        data_dict = {'image': img, 'label': label}\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(data_dict)\n",
    "        print(type(out['prob']))\n",
    "        # Generate XGrad-CAM heatmap\n",
    "        targets = [ClassifierOutputTarget(label.cpu().item())]  # Ensure correct format  \n",
    "        # Make sure the image tensor has the correct shape\n",
    "        if img.dim() == 3:\n",
    "            img = img.unsqueeze(0)  # Convert (C, H, W) → (1, C, H, W)\n",
    "        print(img.shape)\n",
    "        # Run the model (returns a dictionary)\n",
    "        out = model(data_dict)\n",
    "\n",
    "        # Extract raw tensor from the dictionary\n",
    "        out = out['cls']  # ✅ Extract class predictions\n",
    "\n",
    "        # Ensure target is an integer\n",
    "        targets = [ClassifierOutputTarget(out.argmax(dim=1).item())]  # ✅ Get class label\n",
    "        print(targets)\n",
    "\n",
    "        # Run Grad-CAM\n",
    "        grayscale_cam = cam(input_tensor=data_dict['image'], targets=targets)[0]\n",
    "\n",
    "        # Extract first image's CAM\n",
    "        # After Grad-CAM, squeeze only the batch dim\n",
    "        print(grayscale_cam)\n",
    "        grayscale_cam = grayscale_cam.squeeze(0)  # Shape: (224, 224)\n",
    "\n",
    "\n",
    "        # Overlay the heatmap on the image\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255  # Normalize for correct blending\n",
    "        overlay = cv2.addWeighted(img_np, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "        # Ensure LIME gets the correct input (original or overlay?)\n",
    "        fake_pred, pixel_counts = lime_grid_eval(img_np)  # Use original image for LIME\n",
    "\n",
    "        print(fake_pred, pixel_counts)\n",
    "\n",
    "        # Plot the original image with the XGrad-CAM heatmap overlay\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        true_fake_pos = int(img_path.split('_fake_')[1].split('.')[0])\n",
    "\n",
    "        # Original image\n",
    "        ax[0].imshow(img_np)\n",
    "        ax[0].axis('off')\n",
    "        ax[0].set_title(f\"Original Image: {true_fake_pos}\")\n",
    "\n",
    "        # Image with XGrad-CAM heatmap\n",
    "        ax[1].imshow(overlay)\n",
    "        ax[1].axis('off')\n",
    "        ax[1].set_title(f'XGrad-CAM Prediction: {fake_pred}')\n",
    "\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
