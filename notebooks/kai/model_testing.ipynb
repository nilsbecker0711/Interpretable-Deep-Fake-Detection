{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78aa0597-674a-4dbe-be7c-32ffb60b93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:39:38.289652: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 17:39:38.329541: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-31 17:39:38.329574: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-31 17:39:38.329597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 17:39:38.337841: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-31 17:39:39.742411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/pfs/data5/home/ma/ma_ma/ma_kreffert/Jupyter/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'einops.layers.torch.Rearrange'>\n",
      "unknown module type <class 'training.networks.base.vit.PosEmbSinCos2d'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'torch.nn.modules.activation.Softmax'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.Attention'>\n",
      "unknown module type <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "unknown module type <class 'training.networks.base.vit.SimpleViT'>\n",
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "ViTBcosDetector                                         --\n",
      "├─SimpleViT: 1-1                                        --\n",
      "│    └─Sequential: 2-1                                  --\n",
      "│    │    └─Sequential: 3-1                             --\n",
      "│    │    │    └─BcosConv2d: 4-1                        2,592\n",
      "│    │    │    └─LayerNorm: 4-2                         48\n",
      "│    │    │    └─Identity: 4-3                          --\n",
      "│    │    │    └─BcosConv2d: 4-4                        41,472\n",
      "│    │    │    └─LayerNorm: 4-5                         96\n",
      "│    │    │    └─Identity: 4-6                          --\n",
      "│    │    │    └─BcosConv2d: 4-7                        165,888\n",
      "│    │    │    └─LayerNorm: 4-8                         192\n",
      "│    │    │    └─Identity: 4-9                          --\n",
      "│    │    │    └─BcosConv2d: 4-10                       663,552\n",
      "│    │    │    └─LayerNorm: 4-11                        384\n",
      "│    │    │    └─Identity: 4-12                         --\n",
      "│    │    └─Rearrange: 3-2                              --\n",
      "│    │    └─BcosLinear: 3-3                             --\n",
      "│    │    │    └─NormedLinear: 4-13                     147,456\n",
      "│    └─PosEmbSinCos2d: 2-2                              --\n",
      "│    └─Transformer: 2-3                                 --\n",
      "│    │    └─Encoder: 3-4                                --\n",
      "│    │    │    └─Attention: 4-14                        590,208\n",
      "│    │    │    └─FeedForward: 4-15                      1,180,032\n",
      "│    │    └─Encoder: 3-5                                --\n",
      "│    │    │    └─Attention: 4-16                        590,208\n",
      "│    │    │    └─FeedForward: 4-17                      1,180,032\n",
      "│    │    └─Encoder: 3-6                                --\n",
      "│    │    │    └─Attention: 4-18                        590,208\n",
      "│    │    │    └─FeedForward: 4-19                      1,180,032\n",
      "│    │    └─Encoder: 3-7                                --\n",
      "│    │    │    └─Attention: 4-20                        590,208\n",
      "│    │    │    └─FeedForward: 4-21                      1,180,032\n",
      "│    │    └─Encoder: 3-8                                --\n",
      "│    │    │    └─Attention: 4-22                        590,208\n",
      "│    │    │    └─FeedForward: 4-23                      1,180,032\n",
      "│    │    └─Encoder: 3-9                                --\n",
      "│    │    │    └─Attention: 4-24                        590,208\n",
      "│    │    │    └─FeedForward: 4-25                      1,180,032\n",
      "│    │    └─Encoder: 3-10                               --\n",
      "│    │    │    └─Attention: 4-26                        590,208\n",
      "│    │    │    └─FeedForward: 4-27                      1,180,032\n",
      "│    │    └─Encoder: 3-11                               --\n",
      "│    │    │    └─Attention: 4-28                        590,208\n",
      "│    │    │    └─FeedForward: 4-29                      1,180,032\n",
      "│    │    └─Encoder: 3-12                               --\n",
      "│    │    │    └─Attention: 4-30                        590,208\n",
      "│    │    │    └─FeedForward: 4-31                      1,180,032\n",
      "│    │    └─Encoder: 3-13                               --\n",
      "│    │    │    └─Attention: 4-32                        590,208\n",
      "│    │    │    └─FeedForward: 4-33                      1,180,032\n",
      "│    │    └─Encoder: 3-14                               --\n",
      "│    │    │    └─Attention: 4-34                        590,208\n",
      "│    │    │    └─FeedForward: 4-35                      1,180,032\n",
      "│    └─Identity: 2-4                                    --\n",
      "│    └─Sequential: 2-5                                  --\n",
      "│    │    └─LayerNorm: 3-15                             384\n",
      "│    │    └─BcosLinear: 3-16                            --\n",
      "│    │    │    └─NormedLinear: 4-36                     768\n",
      "├─CrossEntropyLoss: 1-2                                 --\n",
      "│    └─CrossEntropyLoss: 2-6                            --\n",
      "================================================================================\n",
      "Total params: 20,495,472\n",
      "Trainable params: 20,495,472\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from utils import load_config\n",
    "import os\n",
    "os.chdir('/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training')\n",
    "print(os.getcwd())\n",
    "from detectors import DETECTOR\n",
    "from torchinfo import summary\n",
    "\n",
    "path = \"./config/detector/vit_bcos.yaml\"\n",
    "additional_args = {'test_batchSize': 4, 'pretrained':None}\n",
    "config = load_config(path, additional_args=additional_args)\n",
    "\n",
    "model_class = DETECTOR[config['model_name']]\n",
    "model = model_class(config)\n",
    "\n",
    "print(summary(model, depth=4))#input_size=(6, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061c7cd6-eed8-44eb-b10a-d4ae81524d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--detector_path DETECTOR_PATH]\n",
      "                             [--train_dataset TRAIN_DATASET [TRAIN_DATASET ...]]\n",
      "                             [--test_dataset TEST_DATASET [TEST_DATASET ...]]\n",
      "                             [--no-save_ckpt] [--no-save_feat] [--ddp]\n",
      "                             [--task_target TASK_TARGET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /pfs/data5/home/ma/ma_ma/ma_kreffert/.local/share/jupyter/runtime/kernel-67c54567-f04b-4f90-9c4f-128b81e65819.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from train import main\n",
    "\n",
    "# Filter out unwanted arguments passed by Jupyter\n",
    "sys.argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
    "\n",
    "# Manually define the arguments you want to pass\n",
    "args = ['--detector_path', '/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/config/detector/vit_bcos.yaml']\n",
    "\n",
    "# Extend sys.argv with your custom arguments\n",
    "sys.argv.extend(args)\n",
    "\n",
    "# Use argparse to parse the arguments\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Process some paths.')\n",
    "    parser.add_argument('--detector_path', type=str, help='path to detector YAML file')\n",
    "    parser.add_argument(\"--train_dataset\", nargs=\"+\")\n",
    "    parser.add_argument(\"--test_dataset\", nargs=\"+\")\n",
    "    parser.add_argument('--no-save_ckpt', dest='save_ckpt', action='store_false', default=True)\n",
    "    parser.add_argument('--no-save_feat', dest='save_feat', action='store_false', default=True)\n",
    "    parser.add_argument(\"--ddp\", action='store_true', default=False)\n",
    "    parser.add_argument('--task_target', type=str, default=\"\", help='specify the target of current training task')\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "# Call parse_args() to parse the known and unknown arguments\n",
    "args, unknown = parse_args()\n",
    "\n",
    "# Now you can pass the args to the main function\n",
    "main()  # The main function will be called with the modified arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b95d8c-1d01-4b98-bbf9-a1e3b5eed20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "SimpleViT                                          --\n",
      "├─Sequential: 1-1                                  --\n",
      "│    └─Sequential: 2-1                             --\n",
      "│    │    └─BcosConv2d: 3-1                        2,592\n",
      "│    │    └─LayerNorm: 3-2                         48\n",
      "│    │    └─Identity: 3-3                          --\n",
      "│    │    └─BcosConv2d: 3-4                        41,472\n",
      "│    │    └─LayerNorm: 3-5                         96\n",
      "│    │    └─Identity: 3-6                          --\n",
      "│    │    └─BcosConv2d: 3-7                        165,888\n",
      "│    │    └─LayerNorm: 3-8                         192\n",
      "│    │    └─Identity: 3-9                          --\n",
      "│    │    └─BcosConv2d: 3-10                       663,552\n",
      "│    │    └─LayerNorm: 3-11                        384\n",
      "│    │    └─Identity: 3-12                         --\n",
      "│    └─Rearrange: 2-2                              --\n",
      "│    └─BcosLinear: 2-3                             --\n",
      "│    │    └─NormedLinear: 3-13                     147,456\n",
      "├─PosEmbSinCos2d: 1-2                              --\n",
      "├─Transformer: 1-3                                 --\n",
      "│    └─Encoder: 2-4                                --\n",
      "│    │    └─Attention: 3-14                        590,208\n",
      "│    │    └─FeedForward: 3-15                      1,180,032\n",
      "│    └─Encoder: 2-5                                --\n",
      "│    │    └─Attention: 3-16                        590,208\n",
      "│    │    └─FeedForward: 3-17                      1,180,032\n",
      "│    └─Encoder: 2-6                                --\n",
      "│    │    └─Attention: 3-18                        590,208\n",
      "│    │    └─FeedForward: 3-19                      1,180,032\n",
      "│    └─Encoder: 2-7                                --\n",
      "│    │    └─Attention: 3-20                        590,208\n",
      "│    │    └─FeedForward: 3-21                      1,180,032\n",
      "│    └─Encoder: 2-8                                --\n",
      "│    │    └─Attention: 3-22                        590,208\n",
      "│    │    └─FeedForward: 3-23                      1,180,032\n",
      "│    └─Encoder: 2-9                                --\n",
      "│    │    └─Attention: 3-24                        590,208\n",
      "│    │    └─FeedForward: 3-25                      1,180,032\n",
      "│    └─Encoder: 2-10                               --\n",
      "│    │    └─Attention: 3-26                        590,208\n",
      "│    │    └─FeedForward: 3-27                      1,180,032\n",
      "│    └─Encoder: 2-11                               --\n",
      "│    │    └─Attention: 3-28                        590,208\n",
      "│    │    └─FeedForward: 3-29                      1,180,032\n",
      "│    └─Encoder: 2-12                               --\n",
      "│    │    └─Attention: 3-30                        590,208\n",
      "│    │    └─FeedForward: 3-31                      1,180,032\n",
      "│    └─Encoder: 2-13                               --\n",
      "│    │    └─Attention: 3-32                        590,208\n",
      "│    │    └─FeedForward: 3-33                      1,180,032\n",
      "│    └─Encoder: 2-14                               --\n",
      "│    │    └─Attention: 3-34                        590,208\n",
      "│    │    └─FeedForward: 3-35                      1,180,032\n",
      "├─Identity: 1-4                                    --\n",
      "├─Sequential: 1-5                                  --\n",
      "│    └─LayerNorm: 2-15                             384\n",
      "│    └─BcosLinear: 2-16                            --\n",
      "│    │    └─NormedLinear: 3-36                     768\n",
      "===========================================================================\n",
      "Total params: 20,495,472\n",
      "Trainable params: 20,495,472\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_25429926/ipykernel_239986/315545080.py:487: UserWarning: You are trying to use the entrypoints from `bcos.models.vit` directly.\n",
      "This is strongly discouraged as it might cause unintended silent errors.\n",
      "Prefer to use the entrypoints from `bcos.models.pretrained` or `torch.hub`.\n",
      "See lines 17-29 of this file () for why.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model2 = vitc_s_patch1_14()\n",
    "print(summary(model2, depth=3))#input_size=(6, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c058e4-92ee-4f52-bd20-b47d2f3e7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains:\n",
    "\n",
    "- Simple ViT\n",
    "- Simple ViT-C (i.e. ViT with a convolutional stem)\n",
    "- B-cos\n",
    "\n",
    "Code taken from lucidrain's vit-pytorch:\n",
    "https://github.com/lucidrains/vit-pytorch/blob/b3e90a265284ba4df00e19fe7a1fd97ba3e3c113/vit_pytorch/simple_vit.py\n",
    "\n",
    "Paper references\n",
    "----------------\n",
    "- Simple ViT: https://arxiv.org/abs/2205.01580\n",
    "- Simple ViT-C: https://arxiv.org/abs/2106.14881\n",
    "- B-cos: https://arxiv.org/abs/2205.10268\n",
    "\n",
    "Note\n",
    "----\n",
    "This is compatible with both a non-B-cos SimpleViT and a B-cos SimpleViT,\n",
    "provided that the correct arguments are passed.\n",
    "\n",
    "Warning\n",
    "-------\n",
    "It is strongly recommended to use the entrypoints defined from `bcos.models.pretrained`\n",
    "or the `torch.hub` interface to load models, instead of using this directly.\n",
    "Especially for B-cos models, as they require a LogitBias module at the end of the model,\n",
    "which the entrypoints below do not include.\n",
    "Feel free to open up an issue at https://github.com/B-cos/B-cos-v2 if you have any questions.\n",
    "\"\"\"\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from bcos.modules.common import DetachableModule\n",
    "from bcos.modules import BcosLinear\n",
    "\n",
    "__all__ = [\n",
    "    \"SimpleViT\",\n",
    "    # entrypoints\n",
    "    \"vitc_ti_patch1_14\",\n",
    "    \"vitc_s_patch1_14\",\n",
    "    \"vitc_b_patch1_14\",\n",
    "    \"vitc_l_patch1_14\",\n",
    "    \"simple_vit_ti_patch16_224\",\n",
    "    \"simple_vit_s_patch16_224\",\n",
    "    \"simple_vit_b_patch16_224\",\n",
    "    \"simple_vit_l_patch16_224\",\n",
    "]\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(x: Any) -> bool:\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def pair(t: Any) -> Tuple[Any, Any]:\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "# classes\n",
    "class PosEmbSinCos2d(nn.Module):\n",
    "    def __init__(self, temperature: Union[int, float] = 10_000):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, patches: Tensor) -> Tensor:\n",
    "        h, w, dim = patches.shape[-3:]\n",
    "        device = patches.device\n",
    "        dtype = patches.dtype\n",
    "\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.arange(h, device=device),\n",
    "            torch.arange(w, device=device),\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "        assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "        omega = torch.arange(dim // 4, device=device) / (dim // 4 - 1)\n",
    "        omega = 1.0 / (self.temperature**omega)\n",
    "\n",
    "        y = y.flatten()[:, None] * omega[None, :]\n",
    "        x = x.flatten()[:, None] * omega[None, :]\n",
    "        pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "        return pe.type(dtype)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        linear_layer: Callable[..., nn.Module] = None,\n",
    "        norm_layer: Callable[..., nn.Module] = None,\n",
    "        act_layer: Callable[..., nn.Module] = None,\n",
    "    ):\n",
    "        assert exists(linear_layer), \"Provide a linear layer class!\"\n",
    "        assert exists(norm_layer), \"Provide a norm layer (compatible with LN) class!\"\n",
    "        assert exists(act_layer), \"Provide a activation layer class!\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                norm=norm_layer(dim),\n",
    "                linear1=linear_layer(dim, hidden_dim),\n",
    "                act=act_layer(),\n",
    "                linear2=linear_layer(hidden_dim, dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(DetachableModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int = 8,\n",
    "        dim_head: int = 64,\n",
    "        linear_layer: nn.Module = None,\n",
    "        norm_layer: nn.Module = None,\n",
    "    ):\n",
    "        assert exists(linear_layer), \"Provide a linear layer class!\"\n",
    "        assert exists(norm_layer), \"Provide a norm layer (compatible with LN) class!\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.att = None\n",
    "\n",
    "        n_lins = 3\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.norm = norm_layer(dim)\n",
    "        self.pos_info = None\n",
    "        self.attention_biases = None\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * n_lins, bias=False)\n",
    "        self.to_out = linear_layer(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), qkv)\n",
    "\n",
    "        if self.detach:  # detach dynamic linear weights\n",
    "            q = q.detach()\n",
    "            k = k.detach()\n",
    "            # these are used for dynamic linear w (`attn` below)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int,\n",
    "        dim_head: int,\n",
    "        mlp_dim: int,\n",
    "        linear_layer: Callable[..., nn.Module] = None,\n",
    "        norm_layer: Callable[..., nn.Module] = None,\n",
    "        act_layer: Callable[..., nn.Module] = None,\n",
    "    ):\n",
    "        assert exists(linear_layer), \"Provide a linear layer class!\"\n",
    "        assert exists(norm_layer), \"Provide a norm layer (compatible with LN) class!\"\n",
    "        assert exists(act_layer), \"Provide a activation layer class!\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            linear_layer=linear_layer,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(\n",
    "            dim,\n",
    "            mlp_dim,\n",
    "            linear_layer=linear_layer,\n",
    "            norm_layer=norm_layer,\n",
    "            act_layer=act_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.attn(x) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        dim_head: int,\n",
    "        mlp_dim: int,\n",
    "        linear_layer: Callable[..., nn.Module] = None,\n",
    "        norm_layer: Callable[..., nn.Module] = None,\n",
    "        act_layer: Callable[..., nn.Module] = None,\n",
    "    ):\n",
    "        assert exists(linear_layer), \"Provide a linear layer class!\"\n",
    "        assert exists(norm_layer), \"Provide a norm layer (compatible with LN) class!\"\n",
    "        assert exists(act_layer), \"Provide a activation layer class!\"\n",
    "\n",
    "        layers_odict = OrderedDict()\n",
    "        for i in range(depth):\n",
    "            layers_odict[f\"encoder_{i}\"] = Encoder(\n",
    "                dim=dim,\n",
    "                heads=heads,\n",
    "                dim_head=dim_head,\n",
    "                mlp_dim=mlp_dim,\n",
    "                linear_layer=linear_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "            )\n",
    "        super().__init__(layers_odict)\n",
    "\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size: Union[int, Tuple[int, int]],\n",
    "        patch_size: Union[int, Tuple[int, int]],\n",
    "        num_classes: int,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        mlp_dim: int,\n",
    "        channels: int = 6,\n",
    "        linear_layer: Callable[..., nn.Module] = None,\n",
    "        norm_layer: Callable[..., nn.Module] = None,\n",
    "        act_layer: Callable[..., nn.Module] = None,\n",
    "        norm2d_layer: Callable[..., nn.Module] = None,\n",
    "        conv2d_layer: Callable[..., nn.Module] = None,\n",
    "        conv_stem: List[int] = None,  # Output channels for each layer of conv stem\n",
    "        **kwargs,\n",
    "    ):\n",
    "        _warn_if_not_called_from_bcos_models_pretrained_or_torch_hub()\n",
    "        super().__init__()\n",
    "        _ = kwargs  # Ignore additional experiment parameters...\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        linear_layer = BcosLinear #(b=b, max_out=max_out) # in_features: int, out_features: int, bias: bool = False, device=None, dtype=None,\n",
    "        conv2d_layer = BcosConv2d #(b=b, max_out=1) # vit_config.get('conv2d_layer', None)\n",
    "\n",
    "        norm_layer = norms.NoBias(nn.LayerNorm) #norms.NoBias(norms.DetachablePositionNorm2d)#norms.NoBias(norms.BatchNormUncentered2d) #vit_config.get('norm_layer', None)\n",
    "        norm2d_layer = norms.NoBias(nn.LayerNorm) #norms.NoBias(norms.DetachablePositionNorm2d)#vit_config.get('norm2d_layer', None)\n",
    "        act_layer = nn.Identity #vit_config.get\n",
    "        \n",
    "        assert exists(linear_layer), \"Provide a linear layer class!\"\n",
    "        assert exists(norm_layer), \"Provide a norm layer (compatible with LN) class!\"\n",
    "        assert exists(act_layer), \"Provide a activation layer class!\"\n",
    "        if conv_stem:\n",
    "            assert exists(\n",
    "                conv2d_layer\n",
    "            ), \"Provide a conv2d layer class when using conv_stem!\"\n",
    "            assert exists(\n",
    "                norm2d_layer\n",
    "            ), \"Provide a norm2d layer class when using conv_stem!\"\n",
    "\n",
    "        assert (\n",
    "            image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "        ), \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        self.num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        self.patch_dim = (\n",
    "            (channels if conv_stem is None else conv_stem[-1])\n",
    "            * patch_height\n",
    "            * patch_width\n",
    "        )\n",
    "        stem = (\n",
    "            dict()\n",
    "            if conv_stem is None\n",
    "            else dict(\n",
    "                conv_stem=make_conv_stem(\n",
    "                    channels, conv_stem, conv2d_layer, norm2d_layer, act_layer\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                **stem,\n",
    "                rearrage=Rearrange(\n",
    "                    \"b c (h p1) (w p2) -> b h w (p1 p2 c)\",\n",
    "                    p1=patch_height,\n",
    "                    p2=patch_width,\n",
    "                ),\n",
    "                linear=linear_layer(self.patch_dim, dim),\n",
    "            )\n",
    "        )\n",
    "        self.positional_embedding = PosEmbSinCos2d()\n",
    "\n",
    "        dim_head = dim // heads\n",
    "        self.transformer = Transformer(\n",
    "            dim,\n",
    "            depth,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            mlp_dim,\n",
    "            linear_layer=linear_layer,\n",
    "            norm_layer=norm_layer,\n",
    "            act_layer=act_layer,\n",
    "        )\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                norm=norm_layer(dim),\n",
    "                linear=linear_layer(dim, num_classes),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = self.positional_embedding(x)\n",
    "        x = rearrange(x, \"b ... d -> b (...) d\") + pe\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)\n",
    "\n",
    "\n",
    "def make_conv_stem(\n",
    "    in_channels: int,\n",
    "    out_channels: List[int],\n",
    "    conv2d_layer: Callable[..., nn.Module] = None,\n",
    "    norm2d_layer: Callable[..., nn.Module] = None,\n",
    "    act_layer: Callable[..., nn.Module] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Following the conv stem design in Early Convolutions Help Transformers See Better (Xiao et al.)\n",
    "    \"\"\"\n",
    "    model = []\n",
    "    for outc in out_channels:\n",
    "        conv = conv2d_layer(\n",
    "            in_channels,\n",
    "            outc,\n",
    "            kernel_size=3,\n",
    "            stride=(2 if outc > in_channels else 1),\n",
    "            padding=1,\n",
    "        )\n",
    "        in_channels = outc\n",
    "        norm = norm2d_layer(in_channels)\n",
    "        act = act_layer()\n",
    "        model += [conv, norm, act]\n",
    "    return nn.Sequential(*model)\n",
    "\n",
    "\n",
    "def vitc_ti_patch1_14(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=14,\n",
    "        patch_size=1,\n",
    "        depth=12\n",
    "        - 1,  # Early convs. help transformers see better: reduce depth to account for conv stem for fairness\n",
    "        dim=384 // 2,\n",
    "        heads=6 // 2,\n",
    "        mlp_dim=1536 // 2,\n",
    "        conv_stem=[24, 48, 96, 192],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def vitc_s_patch1_14(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 2)\n",
    "    return SimpleViT(\n",
    "        image_size=14,\n",
    "        patch_size=1,\n",
    "        depth=12\n",
    "        - 1,  # Early convs. help transformers see better: reduce depth to account for conv stem for fairness\n",
    "        dim=384,\n",
    "        heads=6,\n",
    "        mlp_dim=1536,\n",
    "        conv_stem=[48, 96, 192, 384],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def vitc_b_patch1_14(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=14,\n",
    "        patch_size=1,\n",
    "        depth=12\n",
    "        - 1,  # Early convs. help transformers see better: reduce depth to account for conv stem for fairness\n",
    "        dim=384 * 2,\n",
    "        heads=6 * 2,\n",
    "        mlp_dim=1536 * 2,\n",
    "        conv_stem=[64, 128, 128, 256, 256, 512],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def vitc_l_patch1_14(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=14,\n",
    "        patch_size=1,\n",
    "        depth=14\n",
    "        - 1,  # Early convs. help transformers see better: reduce depth to account for conv stem for fairness\n",
    "        dim=1024,\n",
    "        heads=16,\n",
    "        mlp_dim=1024 * 4,\n",
    "        conv_stem=[64, 128, 128, 256, 256, 512],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def simple_vit_s_patch16_224(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        dim=384,\n",
    "        depth=12,\n",
    "        heads=6,\n",
    "        mlp_dim=1536,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def simple_vit_ti_patch16_224(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        dim=384 // 2,\n",
    "        heads=6 // 2,\n",
    "        mlp_dim=1536 // 2,\n",
    "        depth=12,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def simple_vit_b_patch16_224(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        depth=12,\n",
    "        dim=384 * 2,\n",
    "        heads=6 * 2,\n",
    "        mlp_dim=1536 * 2,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def simple_vit_l_patch16_224(**kwargs):\n",
    "    kwargs.setdefault(\"num_classes\", 1_000)\n",
    "    return SimpleViT(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        depth=14,\n",
    "        dim=1024,\n",
    "        heads=16,\n",
    "        mlp_dim=1024 * 4,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def _warn_if_not_called_from_bcos_models_pretrained_or_torch_hub():\n",
    "    \"\"\"\n",
    "    Warns the user if this module is not called from bcos.models.pretrained or torch.hub\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    import warnings\n",
    "\n",
    "    # if this file is not called from bcos.models.pretrained or torch.hub, warn the user\n",
    "    # note: hubconf uses bcos.models.pretrained under the hood\n",
    "    if not any(\"pretrained\" in call.filename for call in inspect.stack()):\n",
    "        warnings.warn(\n",
    "            \"You are trying to use the entrypoints from `bcos.models.vit` directly.\\n\"\n",
    "            \"This is strongly discouraged as it might cause unintended silent errors.\\n\"\n",
    "            \"Prefer to use the entrypoints from `bcos.models.pretrained` or `torch.hub`.\\n\"\n",
    "            f\"See lines 17-29 of this file () for why.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42485264-e442-49ac-9f47-59370472c07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7173939-5edc-4b0a-9915-b0bd72b5c9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f5759-bbd2-4413-9860-c7bce82b319b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a19ddb4-daa7-4036-921e-86af07beaef4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f13dc6-2975-4985-92b9-6db1428dfe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 16:59:02.997947: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 16:59:03.036265: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-31 16:59:03.036296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-31 16:59:03.036320: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 16:59:03.044238: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-31 16:59:04.206853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/pfs/data5/home/ma/ma_ma/ma_kreffert/Jupyter/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registrierte Backbones: dict_keys(['inception_bcos', 'xception', 'resnet34', 'resnet34_bcos_v2', 'resnet34_bcos_v2_minimal', 'vit_bcos', 'convnext_bcos', 'vgg11_v2_bcos'])\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "VGGBcosDetector                          --\n",
      "├─BcosVGG: 1-1                           --\n",
      "│    └─Sequential: 2-1                   --\n",
      "│    │    └─BcosConv2d: 3-1              --\n",
      "│    │    │    └─NormedConv2d: 4-1       3,456\n",
      "│    │    └─Identity: 3-2                --\n",
      "│    │    └─AvgPool2d: 3-3               --\n",
      "│    │    └─BcosConv2d: 3-4              --\n",
      "│    │    │    └─NormedConv2d: 4-2       73,728\n",
      "│    │    └─Identity: 3-5                --\n",
      "│    │    └─AvgPool2d: 3-6               --\n",
      "│    │    └─BcosConv2d: 3-7              --\n",
      "│    │    │    └─NormedConv2d: 4-3       294,912\n",
      "│    │    └─Identity: 3-8                --\n",
      "│    │    └─BcosConv2d: 3-9              --\n",
      "│    │    │    └─NormedConv2d: 4-4       589,824\n",
      "│    │    └─Identity: 3-10               --\n",
      "│    │    └─AvgPool2d: 3-11              --\n",
      "│    │    └─BcosConv2d: 3-12             --\n",
      "│    │    │    └─NormedConv2d: 4-5       1,179,648\n",
      "│    │    └─Identity: 3-13               --\n",
      "│    │    └─BcosConv2d: 3-14             --\n",
      "│    │    │    └─NormedConv2d: 4-6       2,359,296\n",
      "│    │    └─Identity: 3-15               --\n",
      "│    │    └─AvgPool2d: 3-16              --\n",
      "│    │    └─BcosConv2d: 3-17             --\n",
      "│    │    │    └─NormedConv2d: 4-7       2,359,296\n",
      "│    │    └─Identity: 3-18               --\n",
      "│    │    └─BcosConv2d: 3-19             --\n",
      "│    │    │    └─NormedConv2d: 4-8       2,359,296\n",
      "│    │    └─Identity: 3-20               --\n",
      "│    │    └─AvgPool2d: 3-21              --\n",
      "│    └─LogitLayer: 2-2                   --\n",
      "│    └─Sequential: 2-3                   --\n",
      "│    │    └─BcosConv2d: 3-22             --\n",
      "│    │    │    └─NormedConv2d: 4-9       102,760,448\n",
      "│    │    └─BcosConv2d: 3-23             --\n",
      "│    │    │    └─NormedConv2d: 4-10      16,777,216\n",
      "│    │    └─BcosConv2d: 3-24             --\n",
      "│    │    │    └─NormedConv2d: 4-11      8,192\n",
      "├─CrossEntropyLoss: 1-2                  --\n",
      "│    └─CrossEntropyLoss: 2-4             --\n",
      "=================================================================\n",
      "Total params: 128,765,312\n",
      "Trainable params: 128,765,312\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from utils import load_config\n",
    "import os\n",
    "os.chdir('/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training')\n",
    "print(os.getcwd())\n",
    "from detectors import DETECTOR\n",
    "from torchinfo import summary\n",
    "\n",
    "path = \"./config/detector/vgg_bcos.yaml\"\n",
    "additional_args = {'test_batchSize': 4, 'pretrained':False}\n",
    "config = load_config(path, additional_args=additional_args)\n",
    "\n",
    "model_class = DETECTOR[config['model_name']]\n",
    "model = model_class(config)\n",
    "\n",
    "print(summary(model, depth=4))#input_size=(6, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4164209f-4f08-4eee-b262-f52bdc92be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "BcosVGG                                  --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─BcosConv2d: 2-1                   --\n",
      "│    │    └─NormedConv2d: 3-1            3,456\n",
      "│    └─Identity: 2-2                     --\n",
      "│    └─AvgPool2d: 2-3                    --\n",
      "│    └─BcosConv2d: 2-4                   --\n",
      "│    │    └─NormedConv2d: 3-2            73,728\n",
      "│    └─Identity: 2-5                     --\n",
      "│    └─AvgPool2d: 2-6                    --\n",
      "│    └─BcosConv2d: 2-7                   --\n",
      "│    │    └─NormedConv2d: 3-3            294,912\n",
      "│    └─Identity: 2-8                     --\n",
      "│    └─BcosConv2d: 2-9                   --\n",
      "│    │    └─NormedConv2d: 3-4            589,824\n",
      "│    └─Identity: 2-10                    --\n",
      "│    └─AvgPool2d: 2-11                   --\n",
      "│    └─BcosConv2d: 2-12                  --\n",
      "│    │    └─NormedConv2d: 3-5            1,179,648\n",
      "│    └─Identity: 2-13                    --\n",
      "│    └─BcosConv2d: 2-14                  --\n",
      "│    │    └─NormedConv2d: 3-6            2,359,296\n",
      "│    └─Identity: 2-15                    --\n",
      "│    └─AvgPool2d: 2-16                   --\n",
      "│    └─BcosConv2d: 2-17                  --\n",
      "│    │    └─NormedConv2d: 3-7            2,359,296\n",
      "│    └─Identity: 2-18                    --\n",
      "│    └─BcosConv2d: 2-19                  --\n",
      "│    │    └─NormedConv2d: 3-8            2,359,296\n",
      "│    └─Identity: 2-20                    --\n",
      "│    └─AvgPool2d: 2-21                   --\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─BcosConv2d: 2-22                  --\n",
      "│    │    └─NormedConv2d: 3-9            102,760,448\n",
      "│    └─BcosConv2d: 2-23                  --\n",
      "│    │    └─NormedConv2d: 3-10           16,777,216\n",
      "│    └─BcosConv2d: 2-24                  --\n",
      "│    │    └─NormedConv2d: 3-11           8,192\n",
      "├─LogitLayer: 1-3                        --\n",
      "=================================================================\n",
      "Total params: 128,765,312\n",
      "Trainable params: 128,765,312\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "model2 = vgg11(pretrained=False, progress=True, num_classes=2)\n",
    "print(summary(model2, depth=4))#input_size=(6, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46048d74-7235-4c34-bb23-5ab613931ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "B-cos VGG models\n",
    "\n",
    "Modified from https://github.com/pytorch/vision/blob/0504df5ddf9431909130e7788faf05446bb8a2/torchvision/models/vgg.py\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bcos.common import BcosUtilMixin\n",
    "from bcos.modules import BcosConv2d, LogitLayer, norms\n",
    "\n",
    "__all__ = [\n",
    "    \"BcosVGG\",\n",
    "    \"vgg11\",\n",
    "    \"vgg11_bnu\",\n",
    "    \"vgg13\",\n",
    "    \"vgg13_bnu\",\n",
    "    \"vgg16\",\n",
    "    \"vgg16_bnu\",\n",
    "    \"vgg19_bnu\",\n",
    "    \"vgg19\",\n",
    "]\n",
    "\n",
    "\n",
    "DEFAULT_CONV_LAYER = BcosConv2d\n",
    "DEFAULT_NORM_LAYER = norms.NoBias(norms.BatchNormUncentered2d)\n",
    "\n",
    "\n",
    "class BcosVGG(BcosUtilMixin, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: nn.Module,\n",
    "        num_classes: int = 1000,\n",
    "        init_weights: bool = True,\n",
    "        conv_layer: Callable[..., nn.Module] = DEFAULT_CONV_LAYER,\n",
    "        logit_bias: Optional[float] = None,\n",
    "        logit_temperature: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        super(BcosVGG, self).__init__()\n",
    "        self.features = features\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            conv_layer(512, 4096, kernel_size=7, padding=3, scale_fact=1000),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            conv_layer(4096, 4096, scale_fact=1000),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            conv_layer(4096, num_classes, scale_fact=1000),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "        self.logit_layer = LogitLayer(\n",
    "            logit_temperature=logit_temperature,\n",
    "            logit_bias=logit_bias or -math.log(num_classes - 1),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        # x = self.avgpool(x)\n",
    "        # x = torch.flatten(x, 1)[..., None, None]\n",
    "        x = self.classifier(x)\n",
    "        # because it's supposed to come after\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.logit_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_classifier(self) -> nn.Module:\n",
    "        \"\"\"Returns the classifier part of the model. Note this comes before global pooling.\"\"\"\n",
    "        return self.classifier\n",
    "\n",
    "    def get_feature_extractor(self) -> nn.Module:\n",
    "        \"\"\"Returns the feature extractor part of the model. Without global pooling.\"\"\"\n",
    "        return self.features\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(\n",
    "    cfg: List[Union[str, int]],\n",
    "    norm_layer: Callable[..., nn.Module] = DEFAULT_NORM_LAYER,\n",
    "    conv_layer: Callable[..., nn.Module] = DEFAULT_CONV_LAYER,\n",
    "    in_channels: int = 6,\n",
    "    no_pool=False,\n",
    ") -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    new_config = []\n",
    "    for idx, entry in enumerate(cfg):\n",
    "        new_config.append([entry, 1])\n",
    "        if entry == \"M\" and no_pool:\n",
    "            new_config[idx - 1][1] = 2\n",
    "\n",
    "    for v, stride in new_config:\n",
    "        if v == \"M\":\n",
    "            if no_pool:\n",
    "                continue\n",
    "            layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = conv_layer(\n",
    "                in_channels, v, kernel_size=3, padding=1, stride=stride, scale_fact=1000\n",
    "            )\n",
    "            if not isinstance(norm_layer, nn.Identity):\n",
    "                layers += [conv2d, norm_layer(v)]  # , nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"D\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"E\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(\n",
    "    arch: str,\n",
    "    cfg: str,\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    norm_layer: Callable[..., nn.Module] = DEFAULT_NORM_LAYER,\n",
    "    conv_layer: Callable[..., nn.Module] = DEFAULT_CONV_LAYER,\n",
    "    in_chans: int = 6,\n",
    "    no_pool=False,\n",
    "    **kwargs: Any\n",
    ") -> BcosVGG:\n",
    "    if pretrained:\n",
    "        kwargs[\"init_weights\"] = False\n",
    "    model = BcosVGG(\n",
    "        make_layers(\n",
    "            cfgs[cfg],\n",
    "            norm_layer=norm_layer,\n",
    "            conv_layer=conv_layer,\n",
    "            in_channels=in_chans,\n",
    "            no_pool=no_pool,\n",
    "        ),\n",
    "        conv_layer=conv_layer,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if pretrained:\n",
    "        raise ValueError(\n",
    "            \"If you want to load pretrained weights, then please use the entrypoints in \"\n",
    "            \"bcos.pretrained or bcos.model.pretrained instead.\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> BcosVGG:\n",
    "    return _vgg(\"vgg11\", \"A\", pretrained, progress, norm_layer=nn.Identity, **kwargs)\n",
    "\n",
    "\n",
    "def vgg11_bnu(\n",
    "    pretrained: bool = False, progress: bool = True, **kwargs: Any\n",
    ") -> BcosVGG:\n",
    "    return _vgg(\"vgg11_bnu\", \"A\", pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> BcosVGG:\n",
    "    return _vgg(\"vgg13\", \"B\", pretrained, progress, norm_layer=nn.Identity, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bnu(\n",
    "    pretrained: bool = False, progress: bool = True, **kwargs: Any\n",
    ") -> BcosVGG:\n",
    "    return _vgg(\"vgg13_bnu\", \"B\", pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> BcosVGG:\n",
    "    return _vgg(\"vgg16\", \"D\", pretrained, progress, norm_layer=nn.Identity, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bnu(\n",
    "    pretrained: bool = False, progress: bool = True, **kwargs: Any\n",
    ") -> BcosVGG:\n",
    "    return _vgg(\"vgg16_bnu\", \"D\", pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> BcosVGG:\n",
    "    return _vgg(\"vgg19\", \"E\", pretrained, progress, norm_layer=nn.Identity, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bnu(\n",
    "    pretrained: bool = False, progress: bool = True, **kwargs: Any\n",
    ") -> BcosVGG:\n",
    "    return _vgg(\"vgg19_bnu\", \"E\", pretrained, progress, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jupyter)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
