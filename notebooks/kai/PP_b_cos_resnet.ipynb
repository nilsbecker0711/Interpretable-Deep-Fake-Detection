{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "os.chdir('/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 19:27:58,773 - INFO - Save log to ./logs/training/resnet34_bcos_2025-02-04-19-27-58\n",
      "2025-02-04 19:27:58,774 - INFO - --------------- Configuration ---------------\n",
      "2025-02-04 19:27:58,775 - INFO - Parameters: \n",
      "log_dir: ./logs/training/\n",
      "pretrained: /home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/pretrained/bcos/resnet34-333f7ec4.pth\n",
      "from_url: False\n",
      "model_name: resnet34_bcos\n",
      "backbone_name: resnet34_bcos\n",
      "backbone_config: {'num_classes': 2, 'inc': 3, 'dropout': False, 'pretrained': True, 'groups': 1, 'base_width': 64, 'zero_init_residual': False, 'replace_stride_with_dilation': 'None', 'norm_layer': 'None', 'short_cat': False, 'mode': 'Original'}\n",
      "all_dataset: ['FaceForensics++', 'FF-F2F', 'FF-DF', 'FF-FS', 'FF-NT']\n",
      "train_dataset: ['FaceForensics++']\n",
      "test_dataset: ['FaceForensics++']\n",
      "compression: c40\n",
      "train_batchSize: 64\n",
      "test_batchSize: 64\n",
      "workers: 8\n",
      "frame_num: {'train': 32, 'test': 32}\n",
      "resolution: 224\n",
      "with_mask: False\n",
      "with_landmark: False\n",
      "save_ckpt: False\n",
      "save_feat: True\n",
      "add_inverse_channels: True\n",
      "dataset_type: bcos\n",
      "use_data_augmentation: True\n",
      "data_aug: {'flip_prob': 0.5, 'rotate_prob': 0.5, 'rotate_limit': [-10, 10], 'blur_prob': 0.5, 'blur_limit': [3, 7], 'brightness_prob': 0.5, 'brightness_limit': [-0.1, 0.1], 'contrast_limit': [-0.1, 0.1], 'quality_lower': 40, 'quality_upper': 100}\n",
      "mean: [0.5, 0.5, 0.5]\n",
      "std: [0.5, 0.5, 0.5]\n",
      "optimizer: {'type': 'adam', 'adam': {'lr': 0.0002, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False}, 'sgd': {'lr': 0.0002, 'momentum': 0.9, 'weight_decay': 0.0005}, 'gradient_clipping': False}\n",
      "lr_scheduler: step\n",
      "lr_step: 4\n",
      "lr_gamma: 0.2\n",
      "nEpochs: 15\n",
      "start_epoch: 0\n",
      "save_epoch: 1\n",
      "rec_iter: 100\n",
      "logdir: ./logs\n",
      "manualSeed: 1024\n",
      "loss_func: cross_entropy\n",
      "losstype: None\n",
      "metric_scoring: auc\n",
      "ddp: False\n",
      "cuda: False\n",
      "cudnn: False\n",
      "mode: train\n",
      "lmdb: False\n",
      "dry_run: False\n",
      "rgb_dir: \n",
      "lmdb_dir: ./datasets/lmdb\n",
      "dataset_json_folder: preprocessing/dataset_json_v3\n",
      "SWA: False\n",
      "save_avg: True\n",
      "label_dict: {'DFD_fake': 1, 'DFD_real': 0, 'FF-SH': 1, 'FF-F2F': 1, 'FF-DF': 1, 'FF-FS': 1, 'FF-NT': 1, 'FF-FH': 1, 'FF-real': 0, 'CelebDFv1_real': 0, 'CelebDFv1_fake': 1, 'CelebDFv2_real': 0, 'CelebDFv2_fake': 1, 'DFDCP_Real': 0, 'DFDCP_FakeA': 1, 'DFDCP_FakeB': 1, 'DFDC_Fake': 1, 'DFDC_Real': 0, 'DF_fake': 1, 'DF_real': 0, 'UADFV_Fake': 1, 'UADFV_Real': 0, 'roop_Real': 0, 'roop_Fake': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import datetime\n",
    "from logger import create_logger, RankFilter\n",
    "\n",
    "path = \"./config/detector/resnet34_bcos.yaml\"\n",
    "# parse options and load config\n",
    "with open(path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "try:# KAI: added this, to ensure it finds the config file\n",
    "    with open('./training/config/train_config.yaml', 'r') as f:\n",
    "        config2 = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    with open(os.path.expanduser('~/Interpretable-Deep-Fake-Detection/training/config/train_config.yaml'), 'r') as f:\n",
    "        config2 = yaml.safe_load(f)\n",
    "if 'label_dict' in config:\n",
    "    config2['label_dict']=config['label_dict']\n",
    "config.update(config2)\n",
    "# config['local_rank']=args.local_rank\n",
    "if config['dry_run']:\n",
    "    config['nEpochs'] = 0\n",
    "    config['save_feat']=False\n",
    "\n",
    "\n",
    "if config['lmdb']:\n",
    "    config['dataset_json_folder'] = 'preprocessing/dataset_json_v3'\n",
    "# create logger\n",
    "timenow=datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "task_str = f\"_{config['task_target']}\" if config.get('task_target', None) is not None else \"\"\n",
    "logger_path =  os.path.join(\n",
    "            config['log_dir'],\n",
    "            config['model_name'] + task_str + '_' + timenow\n",
    "        )\n",
    "os.makedirs(logger_path, exist_ok=True)\n",
    "logger = create_logger(os.path.join(logger_path, 'training.log'))\n",
    "logger.info('Save log to {}'.format(logger_path))\n",
    "#config['ddp']= args.ddp\n",
    "\n",
    "\n",
    "# print configuration\n",
    "logger.info(\"--------------- Configuration ---------------\")\n",
    "params_string = \"Parameters: \\n\"\n",
    "for key, value in config.items():\n",
    "    params_string += \"{}: {}\".format(key, value) + \"\\n\"\n",
    "logger.info(params_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 19:27:59.177193: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 19:27:59.216812: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-04 19:27:59.216847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-04 19:27:59.216870: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 19:27:59.224923: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-04 19:28:00.323190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-02-04 19:28:01,313 - INFO - Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2025-02-04 19:28:01,314 - INFO - Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "/pfs/data5/home/ma/ma_ma/ma_kreffert/Jupyter/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']\n",
      "spatial_count=0 keep_stride_count=0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
    "sys.argv = [\"train.py\"]\n",
    "from train import init_seed, prepare_training_data, prepare_testing_data, choose_optimizer, choose_scheduler, choose_metric\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from datetime import timedelta\n",
    "from detectors import DETECTOR\n",
    "from trainer.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 19:28:07,123 - INFO - Load pretrained model successfully!\n",
      "2025-02-04 19:28:07,125 - INFO - Running model on cpu, since device=cpu available.\n",
      "2025-02-04 19:28:07,126 - INFO - ===> Epoch[0] start!\n",
      "2025-02-04 19:28:07,168 - INFO - data_dict saved to ./logs/training/resnet34_bcos_2025-02-04-19-27-58/train/FaceForensics++/data_dict_train.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1793 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'local_rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     45\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[0;32m---> 46\u001b[0m     best_metric \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtest_data_loaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===> Epoch[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] end with testing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_scoring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparse_metric_for_print(best_metric)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pfs/data5/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/trainer/trainer.py:353\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, epoch, train_data_loader, test_data_loaders)\u001b[0m\n\u001b[1;32m    350\u001b[0m     train_recorder_loss[name]\u001b[38;5;241m.\u001b[39mupdate(value)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# run tensorboard to visualize the training process\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal_rank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSWA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m (epoch\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswa_start\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdry_run\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'local_rank'"
     ]
    }
   ],
   "source": [
    "# init seed\n",
    "init_seed(config)\n",
    "\n",
    "# set cudnn benchmark if needed\n",
    "config['cudnn'] = False\n",
    "if config['cudnn']:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "config['ddp'] = False\n",
    "if config['ddp']:\n",
    "    # dist.init_process_group(backend='gloo')\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',\n",
    "        timeout=timedelta(minutes=30)\n",
    "    )\n",
    "    logger.addFilter(RankFilter(0))\n",
    "\n",
    "# prepare the training data loader\n",
    "train_data_loader = prepare_training_data(config)\n",
    "\n",
    "# prepare the testing data loader\n",
    "test_data_loaders = prepare_testing_data(config)\n",
    "\n",
    "# prepare the model (detector)\n",
    "model_class = DETECTOR[config['model_name']]\n",
    "model = model_class(config)\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# prepare the optimizer\n",
    "optimizer = choose_optimizer(model, config)\n",
    "\n",
    "# prepare the scheduler\n",
    "scheduler = choose_scheduler(config, optimizer)\n",
    "\n",
    "# prepare the metric\n",
    "metric_scoring = choose_metric(config)\n",
    "\n",
    "# prepare the trainer\n",
    "trainer = Trainer(config, model, optimizer, scheduler, logger, metric_scoring, time_now=timenow)\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "# start training\n",
    "for epoch in range(config['start_epoch'], config['nEpochs'] + 1):\n",
    "    trainer.model.epoch = epoch\n",
    "    best_metric = trainer.train_epoch(\n",
    "                epoch=epoch,\n",
    "                train_data_loader=train_data_loader,\n",
    "                test_data_loaders=test_data_loaders,\n",
    "            )\n",
    "    if best_metric is not None:\n",
    "        logger.info(f\"===> Epoch[{epoch}] end with testing {metric_scoring}: {parse_metric_for_print(best_metric)}!\")\n",
    "logger.info(\"Stop Training on best Testing metric {}\".format(parse_metric_for_print(best_metric))) \n",
    "# # update\n",
    "# if 'svdd' in config['model_name']:\n",
    "#     model.update_R(epoch)\n",
    "# if scheduler is not None:\n",
    "#     scheduler.step()\n",
    "\n",
    "# # close the tensorboard writers\n",
    "# for writer in trainer.writers.values():\n",
    "#     writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -l\n",
    "!scontrol show job 25223887    \n",
    "!sstat -j 25220981 --format=JobID,MaxRSS,AveRSS,AveCPU\n",
    "!free -h  # Memory usage summary\n",
    "!srun --jobid=25223887 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 2412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b_cos.resnet import resnet50\n",
    "from b_cos.bcosconv2d import BcosConv2d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyToTensor(transforms.ToTensor):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Same as transforms.ToTensor, except that if input to __call__ is already tensor, the input is returned unchanged\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, input_img):\n",
    "        if not isinstance(input_img, torch.Tensor):\n",
    "            return super().__call__(input_img)\n",
    "        return input_img\n",
    "\n",
    "class AddInverse(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        \"\"\"\n",
    "            Adds (1-in_tensor) as additional channels to its input via torch.cat().\n",
    "            Can be used for images to give all spatial locations the same sum over the channels to reduce color bias.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        out = torch.cat([in_tensor, 1-in_tensor], self.dim)\n",
    "        return out\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize if necessary\n",
    "    MyToTensor(),            # Converts image to tensor if not already\n",
    "    AddInverse(dim=0),            # Adds the inverse channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_paths (dict): Dictionary where keys are folder paths, and values are the labels.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_files = []  # Store (image_path, label) tuples\n",
    "        for fp, label in folder_paths.items():\n",
    "            # List all subdirectories within the folder\n",
    "            subfolders = [os.path.join(fp, d) for d in os.listdir(fp) if os.path.isdir(os.path.join(fp, d))]\n",
    "            for folder_path in subfolders:\n",
    "                # Collect all image paths from the folder and assign the given label\n",
    "                self.image_files.extend(\n",
    "                    [(os.path.join(folder_path, f), label) for f in os.listdir(folder_path) if f.endswith((\".png\", \".jpg\"))]\n",
    "                )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_files[idx]  # Get image path and its label\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "file_path_deepfakebench = {'../../DeepfakeBench/datasets/rgb/FaceForensics++/manipulated_sequences/DeepFakeDetection/c40/frames': 1,\n",
    "                           \"../../DeepfakeBench/datasets/rgb/FaceForensics++/original_sequences/actors/c40/frames\": 0}\n",
    "\n",
    "dataset = CustomImageDataset(file_path_deepfakebench, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, label in dataloader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, model_url):\n",
    "    # Load the state dict from the URL\n",
    "    state_dict = load_state_dict_from_url(model_url, progress=True)\n",
    "\n",
    "    # Rename keys to match custom model layer names and initialize adapted state dict\n",
    "    adapted_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # Replace \".weight\" with \".linear.weight\" if necessary for `BcosConv2d`\n",
    "        new_key = key.replace(\"conv\", \"conv.linear\").replace(\"fc\", \"fc.linear\")\n",
    "        \n",
    "        # Only add weights if the shape matches the model's layer\n",
    "        if new_key in model.state_dict() and model.state_dict()[new_key].shape == value.shape:\n",
    "            adapted_state_dict[new_key] = value\n",
    "\n",
    "    # Load the adapted state dict with partial loading (non-strict mode)\n",
    "    model.load_state_dict(adapted_state_dict, strict=False)\n",
    "\n",
    "    # Reinitialize the final layer to match the desired output shape (for 2 classes)\n",
    "    nn.init.kaiming_normal_(model.fc.linear.weight)\n",
    "    if model.fc.linear.bias is not None:\n",
    "        model.fc.linear.bias.data.zero_()  # Initialize bias to zero if it exists\n",
    "\n",
    "# Usage with resnet50\n",
    "model = resnet50(pretrained=False, progress=True, num_classes=1, groups=32, width_per_group=4)\n",
    "load_pretrained_weights(model, 'https://download.pytorch.org/models/resnet50-19c8e357.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "fraction = 1\n",
    "num_batches = int(len(dataloader) * fraction)\n",
    "\n",
    "# Training loop with limited batches\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, total=num_batches, desc=f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "        if batch_idx >= num_batches:\n",
    "            break  # Stop after reaching the fraction of batches\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # outputs = outputs.view(outputs.size(0), -1) NEEDED WHEN USING CROSS ENTROPY LOSS & num_classes > 1\n",
    "\n",
    "        # Convert labels to float for BCEWithLogitsLoss\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'Loss': running_loss / (batch_idx + 1)})\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / num_batches:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'b_cos_model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = resnet50(pretrained=False, progress=True, num_classes=1, groups=32, width_per_group=4)\n",
    "\n",
    "model_trained.load_state_dict(torch.load('b_cos_model_x.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Converting tensor to numpy.\n",
    "    Args:\n",
    "        tensor: torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "        Tensor converted to numpy.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        return tensor\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def grad_to_img(img, linear_mapping, smooth=15, alpha_percentile=99.5):\n",
    "    \"\"\"\n",
    "    Computing color image from dynamic linear mapping of B-cos models.\n",
    "    Args:\n",
    "        img: Original input image (encoded with 6 color channels)\n",
    "        linear_mapping: linear mapping W_{1\\rightarrow l} of the B-cos model\n",
    "        smooth: kernel size for smoothing the alpha values\n",
    "        alpha_percentile: cut-off percentile for the alpha value\n",
    "\n",
    "    Returns:\n",
    "        image explanation of the B-cos model\n",
    "    \"\"\"\n",
    "    # shape of img and linmap is [C, H, W], summing over first dimension gives the contribution map per location\n",
    "    contribs = (img * linear_mapping).sum(0, keepdim=True)\n",
    "    contribs = contribs[0]\n",
    "    # Normalise each pixel vector (r, g, b, 1-r, 1-g, 1-b) s.t. max entry is 1, maintaining direction\n",
    "    rgb_grad = (linear_mapping / (linear_mapping.abs().max(0, keepdim=True)[0] + 1e-12))\n",
    "    # clip off values below 0 (i.e., set negatively weighted channels to 0 weighting)\n",
    "    rgb_grad = rgb_grad.clamp(0)\n",
    "    # normalise s.t. each pair (e.g., r and 1-r) sums to 1 and only use resulting rgb values\n",
    "    rgb_grad = to_numpy(rgb_grad[:3] / (rgb_grad[:3] + rgb_grad[3:]+1e-12))\n",
    "\n",
    "    # Set alpha value to the strength (L2 norm) of each location's gradient\n",
    "    alpha = (linear_mapping.norm(p=2, dim=0, keepdim=True))\n",
    "    # Only show positive contributions\n",
    "    alpha = torch.where(contribs[None] < 0, torch.zeros_like(alpha) + 1e-12, alpha)\n",
    "    if smooth:\n",
    "        alpha = F.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1)//2)\n",
    "    alpha = to_numpy(alpha)\n",
    "    alpha = (alpha / np.percentile(alpha, alpha_percentile)).clip(0, 1)\n",
    "\n",
    "    rgb_grad = np.concatenate([rgb_grad, alpha], axis=0)\n",
    "    # Reshaping to [H, W, C]\n",
    "    grad_image = rgb_grad.transpose((1, 2, 0))\n",
    "    return grad_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_batch, label_batch in dataloader:\n",
    "    print(f\"Batch of images shape: {img_batch.shape}\")\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    # Iterate through each image in the batch\n",
    "    for i in range(len(img_batch)):\n",
    "        img = img_batch[i].unsqueeze(0).to(device)  # Process a single image\n",
    "        label = label_batch[i]\n",
    "\n",
    "        img = img.requires_grad_(True)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model_trained.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model_trained(img)\n",
    "\n",
    "        # Backward pass\n",
    "        out.backward()\n",
    "\n",
    "        # Generate attention visualization\n",
    "        att = grad_to_img(img[0], img.grad[0], alpha_percentile=100, smooth=5)\n",
    "        att[..., -1] *= to_numpy(out.sigmoid())\n",
    "\n",
    "        # Prepare the image and attention map for visualization\n",
    "        att = to_numpy(att)\n",
    "        img_np = np.array(to_numpy(img[0, :3].permute(1, 2, 0)) * 255, dtype=np.uint8)\n",
    "\n",
    "        # Plot the results\n",
    "        fig, ax = plt.subplots(1, figsize=(8, 4))\n",
    "        plt.imshow(img_np, extent=(0, 224, 0, 224))\n",
    "        plt.imshow(att, extent=(224, 2 * 224, 0, 224))\n",
    "        plt.xlim(0, 2 * 224)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"True Label: {label}, Predictions: {out.sigmoid().item():.2f}\")\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    break  # Exit after processing the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_trained.to(device)\n",
    "\n",
    "# Set model_trained to evaluation mode\n",
    "model_trained.eval()\n",
    "\n",
    "# Lists to store all predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move to device\n",
    "        outputs = model_trained(images)  # Forward pass\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jupyter)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
