{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38873a3e-14ea-4b87-9388-062e347cb5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 17:45:33.533310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-25 17:45:33.546588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745595933.560613 2412123 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745595933.565150 2412123 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 17:45:33.581272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/bwhpc/common/jupyter/ai/2025-02-20/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/bwhpc/common/jupyter/ai/2025-02-20/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdetectors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DETECTOR\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# from trainer.trainer import Trainer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_epoch, test_one_dataset, test_epoch, inference\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n",
      "File \u001b[0;32m/pfs/data6/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/test.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepfakeBcosDataset\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepfakeAbstractBaseDataset\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mff_blend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FFBlendDataset\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfwa_blend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FWABlendDataset\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpair_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairDataset\n",
      "File \u001b[0;32m/pfs/data6/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/dataset/ff_blend.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m T\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_blend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_align_mat_new\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolor_transfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m color_transfer\n",
      "File \u001b[0;32m/pfs/data6/home/ma/ma_ma/ma_kreffert/Interpretable-Deep-Fake-Detection/training/dataset/utils/face_blend.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "from utils import load_config\n",
    "import yaml\n",
    "import os\n",
    "os.chdir('../../../Interpretable-Deep-Fake-Detection/training/')\n",
    "import torch\n",
    "import random\n",
    "# init seed\n",
    "# init_seed(config)\n",
    "\n",
    "torch.manual_seed(34)\n",
    "random.seed(34)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Disable for strict reproducibility\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import sys\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
    "sys.argv = [\"train.py\"]\n",
    "from train import init_seed, prepare_training_data, prepare_testing_data, choose_optimizer, choose_scheduler, choose_metric\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from datetime import timedelta\n",
    "from detectors import DETECTOR\n",
    "# from trainer.trainer import Trainer\n",
    "from test import test_epoch, test_one_dataset, test_epoch, inference\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ced0c5-ac4a-44d6-85d3-c181ae1bb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./config/detector/resnet34_bcos_v2_minimal.yaml\"\n",
    "base_path = '/pfs/work7/workspace/scratch/ma_tischuet-team_project_explainable_deepfakes/weights/'\n",
    "pretrained_paths = [f'{base_path}resnet34_v2_minimal/03_10_16_34_15_ckpt_best.pth',\n",
    "                   f'{base_path}bcos_resnet_minimal_b2_ckpt_best.pth',\n",
    "                   f'{base_path}resnet34_03_18_16_59_57_ckpt_best.pth']\n",
    "additional_args_0 = {'test_batchSize': 8,\n",
    "                  'pretrained': pretrained_paths[0],\n",
    "                  'backbone_config':{'b': 1.25}\n",
    "                  }\n",
    "config_0 = load_config(path, additional_args=additional_args_0)\n",
    "additional_args_1 = {'test_batchSize': 8,\n",
    "                  'pretrained': pretrained_paths[1],\n",
    "                  'backbone_config':{'b': 2}\n",
    "                  }\n",
    "config_1 = load_config(path, additional_args=additional_args_1)\n",
    "path = \"./config/detector/resnet34.yaml\"\n",
    "additional_args_2 = {'test_batchSize': 8,\n",
    "                  'pretrained': pretrained_paths[2],\n",
    "                   'compression': 'c40'\n",
    "                  }\n",
    "config_2 = load_config(path, additional_args=additional_args_2)\n",
    "configs = [config_0, config_1, config_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecadd6-6fee-4ee8-920b-73c40b4032e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "models = []\n",
    "num_batches = 100000\n",
    "for config in configs:\n",
    "    test_data_loaders = prepare_testing_data(config)\n",
    "    # prepare the model (detector)\n",
    "    model_class = DETECTOR[config['model_name']]\n",
    "    model = model_class(config)\n",
    "    state_dict = torch.load(config['pretrained'])\n",
    "    # Remove \"module.\" prefix\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.cuda()#.to(\"cuda:0\")\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    # print(test_data_loaders.keys())\n",
    "    for key in ['FaceForensics++']:#test_data_loaders.keys():\n",
    "        for i, data_dict in tqdm(enumerate(test_data_loaders[key]),total=len(test_data_loaders[key])):\n",
    "            if i< num_batches:\n",
    "                if 'label_spe' in data_dict:\n",
    "                    data_dict.pop('label_spe')  # remove the specific label\n",
    "                data_dict['label'] = torch.where(data_dict['label']!=0, 1, 0)  # fix the label to 0 and 1 only\n",
    "                # move data to GPU elegantly\n",
    "                for key in data_dict.keys():\n",
    "                    if data_dict[key]!=None:\n",
    "                        data_dict[key]=data_dict[key].cuda()\n",
    "                # model forward without considering gradient computation\n",
    "                # print(data_dict)\n",
    "                with torch.no_grad():\n",
    "                    predictions = inference(model, data_dict)\n",
    "                y_prob.append(predictions['prob'].cpu().numpy())\n",
    "                pred = torch.where(predictions['prob']>=0.5, 1, 0)\n",
    "                y_pred.append(pred.cpu().numpy())\n",
    "                y_true.append(data_dict['label'].cpu().numpy())\n",
    "                # print(data_dict['label'])\n",
    "                # print(predictions['prob'])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        y_pred_flat = np.concatenate(y_pred)\n",
    "        y_true_flat = np.concatenate(y_true)\n",
    "        y_prob_flat = np.concatenate(y_prob)\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "        print(f\"Confusion Matrix for model {config['model_name']}:\\n\", cm)\n",
    "        \n",
    "        # Compute Accuracy\n",
    "        accuracy = accuracy_score(y_true_flat, y_pred_flat)\n",
    "        print(f\"Accuracy for model {config['model_name']}: {accuracy:.4f}\")\n",
    "    \n",
    "        # Compute Precision\n",
    "        precision = precision_score(y_true_flat, y_pred_flat)\n",
    "        print(f\"Precision for model {config['model_name']}: {precision:.4f}\")\n",
    "    \n",
    "        # Compute Recall\n",
    "        recall = recall_score(y_true_flat, y_pred_flat)\n",
    "        print(f\"Recall for model {config['model_name']}: {recall:.4f}\")\n",
    "    \n",
    "        # Compute F1 Score\n",
    "        f1 = f1_score(y_true_flat, y_pred_flat)\n",
    "        print(f\"F1 Score for model {config['model_name']}: {f1:.4f}\")\n",
    "    \n",
    "        # Compute ROC AUC (only for binary classification)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_true_flat,\n",
    "                                                y_prob_flat,\n",
    "                                                 pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "        print(f\"AUC {auc}\")\n",
    "        print(f\"EER {eer}\")\n",
    "\n",
    "        # Compute ROC AUC (only for binary classification)\n",
    "        # fpr, tpr, thresholds = metrics.roc_curve(y_true_flat,\n",
    "        #                                         y_pred_flat,\n",
    "        #                                          pos_label=1)\n",
    "        # auc = metrics.auc(fpr, tpr)\n",
    "        # fnr = 1 - tpr\n",
    "        # eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "        # print(f\"AUC {auc}\")\n",
    "        # print(f\"EER {eer}\")\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true_flat, y_prob_flat)\n",
    "            print(f\"ROC AUC for model {config['model_name']}: {roc_auc:.4f}\")\n",
    "        except ValueError:\n",
    "            print(f\"ROC AUC is not computable for model {config['model_name']} (likely due to imbalanced classes)\")\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881359ee-e540-49cf-91e7-69400e123d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from bcos.interpretability import grad_to_img, to_numpy\n",
    "# Assuming you've already set up device, data loaders, and models\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "threshold = 0.1\n",
    "num_plots = 6\n",
    "fig, axes = plt.subplots(num_plots, 5, figsize=(num_plots* 3, num_plots*3))  \n",
    "for n in range(len(configs[:-1])):\n",
    "    if 'b' in configs[n]['backbone_config'].keys():\n",
    "        fig.text(0.25 + (n/len(configs[:-1])), 1.02, f\"b - {configs[n]['backbone_config']['b']}\", ha='center', va='bottom', fontsize=14)\n",
    "# test_data_loaders = prepare_testing_data(configs[0])\n",
    "for model_idx, config in enumerate(configs[:-1]):\n",
    "    # Prepare the model (detector)\n",
    "    model_class = DETECTOR[config['model_name']]\n",
    "    model = model_class(config)\n",
    "    state_dict = torch.load(config['pretrained'])\n",
    "    \n",
    "    # Remove \"module.\" prefix if present in the state_dict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through data loaders and process the first batch\n",
    "    for key in test_data_loaders.keys():\n",
    "        for i, data_dict in tqdm(enumerate(test_data_loaders[key]), total=len(test_data_loaders[key])):\n",
    "            if i < num_plots:\n",
    "                # Process data and ensure labels are binary (0 or 1)\n",
    "                img_batch, label_batch, landmark, mask = (data_dict[k] for k in ['image', 'label', 'landmark', 'mask'])\n",
    "                if 'label_spe' in data_dict:\n",
    "                    data_dict.pop('label_spe')\n",
    "                data_dict['label'] = torch.where(data_dict['label'] != 0, 1, 0)\n",
    "\n",
    "                for key in data_dict.keys():\n",
    "                    if data_dict[key] is not None:\n",
    "                        data_dict[key] = data_dict[key].to(device)\n",
    "\n",
    "                # Take the first image and process it\n",
    "                img = img_batch[0].unsqueeze(0).to(device)  # Process a single image\n",
    "                label = label_batch[0]\n",
    "\n",
    "                # Generate explanation (replace with your actual explanation function)\n",
    "                model.backbone.eval()\n",
    "                explanation = model.backbone.explain(img)\n",
    "\n",
    "                # Apply threshold to the explanation heatmap to binarize it\n",
    "                explanation_map = explanation['explanation'][:, :, :].copy()\n",
    "                explanation_map[:, :, -1] = (explanation_map[:, :, -1] > threshold).astype(np.uint8)  # Convert to 0 or 1 based on threshold\n",
    "                \n",
    "                # Convert the image to numpy format for visualization\n",
    "                img_np = np.array(to_numpy(img[0, [0, 1, 2]].permute(1, 2, 0)) * 255, dtype=np.uint8)\n",
    "\n",
    "                # Select subplot axes based on the model index (for arranging in 2x2 grid)\n",
    "                row_idx = i #model_idx // 2  # Determine row (0 or 1)\n",
    "                col_idx = model_idx*2+1 #* 2 \n",
    "                if model_idx == 0:\n",
    "                    # Plot the original image on the left subplot\n",
    "                    axes[row_idx, model_idx].imshow(img_np, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "                    axes[row_idx, model_idx].set_title(f\"Label {label}\")\n",
    "                    #axes[row_idx, col_idx].set_xlim(0, config['resolution'])\n",
    "                    axes[row_idx, model_idx].set_xticks([])\n",
    "                    axes[row_idx, model_idx].set_yticks([])\n",
    "\n",
    "                # Plot the explanation (heatmap) on the right subplot\n",
    "                axes[row_idx, col_idx].imshow(explanation['explanation'], cmap='jet', alpha=0.5, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "                axes[row_idx, col_idx].set_title(f\"Explained class {explanation['explained_class_idx']} - Prediction: {explanation['prediction']}\")\n",
    "                #axes[row_idx, col_idx].set_xlim(0, config['resolution'])\n",
    "                axes[row_idx, col_idx].set_xticks([])\n",
    "                axes[row_idx, col_idx].set_yticks([])\n",
    "                axes[row_idx, col_idx].text(0.5, .99,  f\"b - {config['backbone_config']['b']}\", color='green', ha='center', va='top', fontsize=8, weight='bold', transform=axes[row_idx, col_idx].transAxes)\n",
    "                \n",
    "                # Plot the explanation (heatmap) on the right subplot\n",
    "                axes[row_idx, col_idx+1].imshow(explanation_map, cmap='jet', alpha=0.5, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "                # axes[row_idx, col_idx+1].set_title(f\"Explained class {explanation['explained_class_idx']} - Prediction: {explanation['prediction']}\")\n",
    "                #axes[row_idx, col_idx].set_xlim(0, config['resolution'])\n",
    "                axes[row_idx, col_idx+1].set_xticks([])\n",
    "                axes[row_idx, col_idx+1].set_yticks([])\n",
    "                axes[row_idx, col_idx+1].text(0.5, .99,  f\"b - {config['backbone_config']['b']}\", color='green', ha='center', va='top', fontsize=8, weight='bold', transform=axes[row_idx, col_idx].transAxes)\n",
    "\n",
    "                bbox = axes[row_idx, col_idx].get_position()  # This is a Bbox object\n",
    "                x_position = (bbox.xmin + bbox.xmax) / 2  # Calculate the center position of the first explanation plot\n",
    "                y_position = bbox.ymax #(bbox.ymin + bbox.ymax) / 2  # Calculate the center position of the first explanation plot\n",
    "                \n",
    "                \n",
    "                # fig.text(x_position, y_position, \"Explanation Maps\", ha='center', va='top', fontsize=12, weight='bold')\n",
    "                \n",
    "                # Hide axes spines\n",
    "                for ax in [axes[row_idx, col_idx], axes[row_idx, col_idx + 1]]:\n",
    "                    for spine in ax.spines.values():\n",
    "                        spine.set_visible(False)\n",
    "\n",
    "            else:\n",
    "                break  # Only process the first 3 batches (or less if break condition met)\n",
    "        else:\n",
    "            break  # Exit if the test data loader finishes\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1)  # Adjust horizontal spacing between columns\n",
    "plt.tight_layout()  # Ensures proper spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012e54a-51b4-4c2e-95c7-0710401a2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_map = explanation['explanation'][:, :, :].copy()\n",
    "explanation_map[:, :, -1] = (explanation_map[:, :, -1] >0.1).astype(np.uint8)  # Convert to 0 or 1 based on threshold\n",
    "                \n",
    "plt.imshow(explanation_map, cmap='jet', alpha=0.5, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "plt.show()\n",
    "# plt.imshow(explanation['explanation'][:, :, :], cmap='jet', alpha=0.5, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe36ea-b766-49ab-9c06-cccffb099af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bcos.interpretability import grad_to_img, to_numpy\n",
    "# Assuming you've already set up device, data loaders, and models\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "num_plots = 6\n",
    "fig, axes = plt.subplots(num_plots, 4, figsize=(num_plots* 3, num_plots*3))  \n",
    "for n in range(len(configs)):\n",
    "    if 'b' in configs[n]['backbone_config'].keys():\n",
    "        fig.text(0.25 + (n/len(configs)), 1.02, f\"b - {configs[n]['backbone_config']['b']}\", ha='center', va='bottom', fontsize=14)\n",
    "test_data_loaders = prepare_testing_data(configs[0])\n",
    "for model_idx, config in enumerate(configs[:-1]):\n",
    "    # Prepare the model (detector)\n",
    "    model_class = DETECTOR[config['model_name']]\n",
    "    model = model_class(config)\n",
    "    state_dict = torch.load(config['pretrained'])\n",
    "    \n",
    "    # Remove \"module.\" prefix if present in the state_dict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through data loaders and process the first batch\n",
    "    for key in test_data_loaders.keys():\n",
    "        for i, data_dict in tqdm(enumerate(test_data_loaders[key]), total=len(test_data_loaders[key])):\n",
    "            if i < num_plots:\n",
    "                # Process data and ensure labels are binary (0 or 1)\n",
    "                img_batch, label_batch, landmark, mask = (data_dict[k] for k in ['image', 'label', 'landmark', 'mask'])\n",
    "                if 'label_spe' in data_dict:\n",
    "                    data_dict.pop('label_spe')\n",
    "                data_dict['label'] = torch.where(data_dict['label'] != 0, 1, 0)\n",
    "\n",
    "                for key in data_dict.keys():\n",
    "                    if data_dict[key] is not None:\n",
    "                        data_dict[key] = data_dict[key].to(device)\n",
    "\n",
    "                # Take the first image and process it\n",
    "                img = img_batch[0].unsqueeze(0).to(device)  # Process a single image\n",
    "                label = label_batch[0]\n",
    "\n",
    "                # Generate explanation (replace with your actual explanation function)\n",
    "                model.backbone.eval()\n",
    "                explanation = model.backbone.explain(img)\n",
    "                \n",
    "                # Convert the image to numpy format for visualization\n",
    "                img_np = np.array(to_numpy(img[0, [0, 1, 2]].permute(1, 2, 0)) * 255, dtype=np.uint8)\n",
    "\n",
    "                # Select subplot axes based on the model index (for arranging in 2x2 grid)\n",
    "                row_idx = i #model_idx // 2  # Determine row (0 or 1)\n",
    "                col_idx = model_idx * 2 \n",
    "\n",
    "                # Plot the original image on the left subplot\n",
    "                axes[row_idx, col_idx].imshow(img_np, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "                axes[row_idx, col_idx].set_title(f\"Label {label}\")\n",
    "                #axes[row_idx, col_idx].set_xlim(0, config['resolution'])\n",
    "                axes[row_idx, col_idx].set_xticks([])\n",
    "                axes[row_idx, col_idx].set_yticks([])\n",
    "\n",
    "                # Plot the explanation (heatmap) on the right subplot\n",
    "                axes[row_idx, col_idx+1].imshow(explanation['explanation'], cmap='jet', alpha=0.5, extent=(0, config['resolution'], 0, config['resolution']))\n",
    "                axes[row_idx, col_idx+1].set_title(f\"Explained class {explanation['explained_class_idx']} - Prediction: {explanation['prediction']}\")\n",
    "                #axes[row_idx, col_idx].set_xlim(0, config['resolution'])\n",
    "                axes[row_idx, col_idx+1].set_xticks([])\n",
    "                axes[row_idx, col_idx+1].set_yticks([])\n",
    "                axes[row_idx, col_idx+1].text(0.5, .99,  f\"b - {config['backbone_config']['b']}\", color='green', ha='center', va='top', fontsize=8, weight='bold', transform=axes[row_idx, col_idx+1].transAxes)\n",
    "\n",
    "                for spine in axes[row_idx, col_idx].spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            else:\n",
    "                break  # Only process the first 3 batches (or less if break condition met)\n",
    "        else:\n",
    "            break  # Exit if the test data loader finishes\n",
    "plt.tight_layout()  # Ensures proper spacing between subplots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter venv)",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
